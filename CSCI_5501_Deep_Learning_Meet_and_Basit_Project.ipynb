{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hC5tzMRjWQ3P"
   },
   "source": [
    "# Custom K-Beams Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_j8McAWlyah"
   },
   "source": [
    "## Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "collapsed": true,
    "id": "OERsYz8PIlNr",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "24d0682f-221a-4838-dded-8590c5bfbdf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting mauve-text\n",
      "  Downloading mauve_text-0.4.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.2)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.50.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.11/dist-packages (from mauve-text) (1.6.1)\n",
      "Collecting faiss-cpu>=1.7.0 (from mauve-text)\n",
      "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.1->mauve-text) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.1->mauve-text) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.1->mauve-text) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.30.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mauve_text-0.4.0-py3-none-any.whl (21 kB)\n",
      "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=9d5211228cc526e79817e88a2a11800c45def646904964395acc10a482dbe7a1\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, rouge-score, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, mauve-text, bert-score\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bert-score-0.3.13 faiss-cpu-1.10.0 mauve-text-0.4.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score bert-score mauve-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "collapsed": true,
    "id": "yrufLD0sVzwc",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "8bde5146-5666-48b0-eb94-c79ca141fde7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, portalocker, fsspec, dill, colorama, sacrebleu, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed colorama-0.4.6 datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 portalocker-3.1.1 sacrebleu-2.5.1 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "collapsed": true,
    "id": "NBkaTn3WvSpA",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "734ba7f1-636a-4f0e-b148-ed70d8bb6e84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in /usr/local/lib/python3.11/dist-packages (0.30.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.13.1)\n",
      "Collecting hf-xet>=0.1.4 (from huggingface_hub[hf_xet])\n",
      "  Downloading hf_xet-1.0.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2025.1.31)\n",
      "Downloading hf_xet-1.0.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf-xet\n",
      "Successfully installed hf-xet-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yBdMpG4WTw4"
   },
   "source": [
    "## Initialization Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380,
     "referenced_widgets": [
      "6e4a4552c78242f28584c48c33a5eb04",
      "b36366f14d2346e2ba2b25cb097cf4cb",
      "f194e60bb9f6476a84638353628933e4",
      "8625d006acc5487fbd739e43e1b3cc67",
      "30ad066145854ef390ecb9bf11907c65",
      "43b72bb5c9e343598be4bf81e87bee04",
      "d3337d23e66b4daeb5397a3b2189915c",
      "4a1ebcadf3cb469cbbe89f2b44717074",
      "c3722628b79447f694b2b59ffd47eb50",
      "120beb81ef2346ce84e868c33d6e8e53",
      "372d38104c6c49378ad70fb110cd08e8",
      "c9f19fd04e914fefb42ffd7e5f101eb0",
      "6897c62417c247b6abb65abc4b8810eb",
      "82177b03c5a94a1fb5d2ca6e064cd0ae",
      "d17e9315773949c785778d56a2f14fbe",
      "463c7de9ade1432aac13ab577ce300bf",
      "ba48aa41313e4e13952f32ac8bf80e3d",
      "9074cc2ca2484b449bec9510a303990d",
      "cfeda3e43b924e0699909423cea7f797",
      "aff24c2ed7864c19ac5a886c26ad6b9b",
      "58381a39a3794640bf89befb9a8ce821",
      "effbeb339484439993c98514a461384b",
      "f499b83f56cc4f4ebc6176f562fb53b3",
      "b998ae41eaef4fdfaff7f5b95dc8328c",
      "de8f20ed4ada4ca49d525b3d9ce0019b",
      "3634244b592c4dc7b0ed75c26f8f254b",
      "2f86ca69222649e0b992b514fa9e9b6c",
      "edde34e6ea134c6f9c9662d27dc44fe5",
      "5a12667dfeb74686b82af0fef322fdc6",
      "ec15443b964a4f7b8c651c1175912c1a",
      "cff4ef42461e4df2ae0b08ac54714727",
      "bc274256b1fc4587be88480e896680af",
      "0995617c78b645a29ef7bfbeab4dfb28",
      "1b990d64f15245fea905ca4b697808f8",
      "2d9060918271463fa5090500608f056e",
      "c2423b7fe6a142f88fcd68f0e6047fe8",
      "e27d7265ca674a7dbb305aaa8ca5a729",
      "e260ef134ef24385b113a7e063aa9f18",
      "fb9619a387af4157a6e5e02680abfad6",
      "df8cb5ec622d432aaea59006be118cc4",
      "de59170da94b4a0ab046dff88dd2581d",
      "730eac47e4dc4f56b682ebe24b1115f6",
      "574cd2f83c5c422297b16ecf4ec9ec83",
      "3835004e05b84ab9b3322cb11e72bb0a",
      "78de3702e5e34862af75b321d52dce49",
      "80d591b0215242cdb6827358b6301e8a",
      "ad9d62cbdd0d4f4a8cedcf0093b70d06",
      "5d07c8d8c9c64f14ba6f31aaae31291f",
      "5250953aa480416db78c56f9e1b78d99",
      "35a458f2061b468b992c59b7aac88c25",
      "b2fa2acebf0041b692b482fff0f02470",
      "f8f64cf17975468d82d89c6b01276735",
      "1b8a82d566764813948a74e23687ad60",
      "658c836a10604b9cbbdead803f8351a1",
      "b394bcaa58264533b73ea0c5499ae657",
      "5c8554b69c11431581d17117e2e69718",
      "a6259ccfe5e44d24a40c94e675fd9849",
      "f1ec47f22a9a4a6694df4494382847ea",
      "62c55d76dce84033a614577581bd4a20",
      "198d3ee431c048bca36bc1638e720fbc",
      "712aeb8ec3324eb49d9c086f7ebb161a",
      "c7d8f00742b74604a8a6dd51631de46c",
      "7e1f2f217d854770a333444912db451a",
      "5d17bca201684ba1a50c879411a0dae2",
      "6a536805e9564ebe99d98f25198ca537",
      "ba03e173558b464aaa88d21686d29f05",
      "c87e2bad6e0647ee9a22f5929bc3ec7b",
      "cb7062699edf449ca4e603e81f704f7b",
      "5f1600e7908e4b769c6ecac1100a35ac",
      "2918fada2a564367b03729fb200179df",
      "711f3433e0c04d3ab2c7998d5ce01e49",
      "d6fe0858f0f94ae697d19f87ea44f9f5",
      "89047b560397474fa243293a93074822",
      "d319fa3eb1644755ac1c93db5dd4882d",
      "c6cfa29680ce49b29a820f6b311e7458",
      "1bc3a080da5f4f7abb08045530eeaff0",
      "d883d0736a8f4faba988f764a6fbcaee"
     ]
    },
    "id": "irWbf-2zVLLL",
    "outputId": "662370e6-e3ea-4712-99fc-8202dd46f1bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4a4552c78242f28584c48c33a5eb04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f19fd04e914fefb42ffd7e5f101eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f499b83f56cc4f4ebc6176f562fb53b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b990d64f15245fea905ca4b697808f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78de3702e5e34862af75b321d52dce49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8554b69c11431581d17117e2e69718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87e2bad6e0647ee9a22f5929bc3ec7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Once upon a time\n",
      "Output: Once upon a time there was a great deal of confusion about the meaning of the word, and the meaning of the word\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional, Union, Any\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "@dataclass\n",
    "class Beam:\n",
    "    \"\"\"Class to represent a beam in the k-beams search.\"\"\"\n",
    "    sequence: List[int]  # Token IDs in the sequence\n",
    "    score: float         # Cumulative log probability score\n",
    "    window_size: int     # Current window size before dropping\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Sequence: {self.sequence}, Score: {self.score}, Window: {self.window_size}\"\n",
    "\n",
    "class CustomKBeamSearch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Any,\n",
    "        tokenizer: Any,\n",
    "        k: int = 5,\n",
    "        initial_window_size: int = 3,\n",
    "        max_length: int = 20,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the custom k-beams search.\n",
    "\n",
    "        Args:\n",
    "            model: Any model that can generate logits for the next token\n",
    "            tokenizer: Tokenizer for encoding/decoding\n",
    "            k: Beam size\n",
    "            initial_window_size: Window size before dropping a beam not in top-k\n",
    "            max_length: Maximum length of generated sequences\n",
    "            device: Device to run the model on\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.k = k\n",
    "        self.initial_window_size = initial_window_size\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "\n",
    "    def _get_next_token_probabilities(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get probabilities for the next token.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Input token IDs [batch_size, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Log probabilities for next token [batch_size, vocab_size]\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = self.model(input_ids)\n",
    "\n",
    "            # Handle different model output formats\n",
    "            if hasattr(outputs, \"logits\"):\n",
    "                # Take last token's logits\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "            elif isinstance(outputs, tuple) and hasattr(outputs[0], \"logits\"):\n",
    "                logits = outputs[0].logits[:, -1, :]\n",
    "            else:\n",
    "                # Assuming the model directly returns logits\n",
    "                logits = outputs[:, -1, :]\n",
    "\n",
    "            # Convert to log probabilities\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        **model_kwargs\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Generate a sequence using custom k-beams search.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Input token IDs [batch_size=1, seq_len]\n",
    "            attention_mask: Attention mask [batch_size=1, seq_len]\n",
    "            model_kwargs: Additional arguments for the model\n",
    "\n",
    "        Returns:\n",
    "            Generated token IDs\n",
    "        \"\"\"\n",
    "        # Initialize beams with the input sequence [seq_len]\n",
    "        input_seq = input_ids[0].tolist()\n",
    "\n",
    "        # Start with a single beam containing the input sequence\n",
    "        beams = [Beam(sequence=input_seq, score=0.0, window_size=self.initial_window_size)]\n",
    "\n",
    "        # Generate tokens up to max_length\n",
    "        for _ in range(self.max_length):\n",
    "            if not beams:\n",
    "                # If all beams were dropped\n",
    "                break\n",
    "\n",
    "            # Get all current sequences from beams\n",
    "            sequences = [beam.sequence for beam in beams]\n",
    "            scores = [beam.score for beam in beams]\n",
    "            window_sizes = [beam.window_size for beam in beams]\n",
    "\n",
    "            # Prepare input for model\n",
    "            batch_input_ids = torch.tensor([seq for seq in sequences], device=self.device)\n",
    "\n",
    "            # Get next token probabilities for all beams [batch_size, vocab_size]\n",
    "            next_token_log_probs = self._get_next_token_probabilities(batch_input_ids)\n",
    "\n",
    "            # Flatten beams and token probabilities for selection\n",
    "            all_next_tokens = []\n",
    "            all_scores = []\n",
    "            all_beam_indices = []\n",
    "\n",
    "            # For each beam, get top-k next tokens\n",
    "            for i, (beam_log_probs, beam_score) in enumerate(zip(next_token_log_probs, scores)):\n",
    "                # Get top 2k tokens to ensure diversity (we'll filter to k later)\n",
    "                # This gives us a chance to see tokens that might not be in the global top-k\n",
    "                topk_log_probs, topk_indices = torch.topk(beam_log_probs, min(2 * self.k, beam_log_probs.size(-1)))\n",
    "\n",
    "                for token_idx, token_log_prob in zip(topk_indices.tolist(), topk_log_probs.tolist()):\n",
    "                    all_next_tokens.append(token_idx)\n",
    "                    all_scores.append(beam_score + token_log_prob)\n",
    "                    all_beam_indices.append(i)\n",
    "\n",
    "            # Get top-k candidates among all beams\n",
    "            if len(all_scores) <= self.k:\n",
    "                top_indices = list(range(len(all_scores)))\n",
    "            else:\n",
    "                top_scores = torch.tensor(all_scores, device=self.device)\n",
    "                _, top_indices = torch.topk(top_scores, min(self.k, len(all_scores)))\n",
    "                top_indices = top_indices.tolist()\n",
    "\n",
    "            # Track which beams are in top-k\n",
    "            beams_in_topk = set()\n",
    "            for idx in top_indices:\n",
    "                beams_in_topk.add(all_beam_indices[idx])\n",
    "\n",
    "            # Create new candidate beams\n",
    "            new_beams = []\n",
    "            processed_beam_indices = set()\n",
    "\n",
    "            # First, process beams that are in top-k\n",
    "            for idx in top_indices:\n",
    "                beam_idx = all_beam_indices[idx]\n",
    "                token_idx = all_next_tokens[idx]\n",
    "                score = all_scores[idx]\n",
    "\n",
    "                processed_beam_indices.add(beam_idx)\n",
    "\n",
    "                # Create a new beam with the token appended\n",
    "                new_sequence = sequences[beam_idx] + [token_idx]\n",
    "                new_beams.append(Beam(\n",
    "                    sequence=new_sequence,\n",
    "                    score=score,\n",
    "                    window_size=self.initial_window_size  # Reset window for beams in top-k\n",
    "                ))\n",
    "\n",
    "            # Process beams that are not in top-k but still have window remaining\n",
    "            for i, beam in enumerate(beams):\n",
    "                if i not in beams_in_topk and i not in processed_beam_indices:\n",
    "                    # Reduce window size for this beam\n",
    "                    new_window_size = window_sizes[i] - 1\n",
    "\n",
    "                    if new_window_size > 0:\n",
    "                        # Keep this beam but with reduced window\n",
    "                        # We'll use the best token from this beam even though it's not in global top-k\n",
    "                        token_log_probs = next_token_log_probs[i]\n",
    "                        token_score, token_idx = torch.max(token_log_probs, dim=-1)\n",
    "\n",
    "                        new_sequence = sequences[i] + [token_idx.item()]\n",
    "                        new_beams.append(Beam(\n",
    "                            sequence=new_sequence,\n",
    "                            score=scores[i] + token_score.item(),\n",
    "                            window_size=new_window_size\n",
    "                        ))\n",
    "\n",
    "            # Update beams\n",
    "            beams = new_beams\n",
    "\n",
    "            # Remove extinguished beams\n",
    "            beams = [beam for beam in beams if beam.window_size > 0]\n",
    "\n",
    "            # Check if all beams end with EOS token\n",
    "            if self.tokenizer.eos_token_id is not None:\n",
    "                if all(beam.sequence[-1] == self.tokenizer.eos_token_id for beam in beams):\n",
    "                    break\n",
    "\n",
    "        # Return the best beam's sequence\n",
    "        if beams:\n",
    "            best_beam = max(beams, key=lambda x: x.score)\n",
    "            return best_beam.sequence\n",
    "        else:\n",
    "            return input_seq  # Return input if all beams were dropped\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define search parameters\n",
    "beam_search = CustomKBeamSearch(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    k=5,\n",
    "    initial_window_size=3,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "# Prepare input\n",
    "prompt = \"Once upon a time\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate sequence\n",
    "output_ids = beam_search.generate(input_ids.to(model.device))\n",
    "\n",
    "# Decode output\n",
    "output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mc3KjkqDWMO1"
   },
   "source": [
    "## Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1815,
     "referenced_widgets": [
      "036613db532c4ac19bc597a930cfd0e0",
      "5e2b7d0ef400493583565505f9261338",
      "e0845243ba3e40f6a39078463e89f8e9",
      "a58326aa19ed4df5afa070eb8d1f1beb",
      "0880863fe8ec40dc8623647d1f89fbf5",
      "e6c92d2a86db466e92d07ea1923a553b",
      "3b9b9280dc8540708bcd6933eb157217",
      "36fbd7916be7470e84166c97759fd8bc",
      "b84cad83c8ca44819ddd3fbdc0557203",
      "4be89621da8443688905836b492a70c1",
      "93f419f6d90b49919c8e00118bac1311",
      "ff687bfaa4be49e0a2fabb62fd023db8",
      "0433072baac0455c8bb3cf566ae8000d",
      "66bbe272676048df9a3084f5b553c9fa",
      "f638f3abee65447e83d71e22bc92e397",
      "97abfb05deb5492f8fdf0e302ae932ca",
      "bd893e023483439695973f7af7d204ba",
      "47b4dc443c164ac3b8562d27624cfc40",
      "cb2b9528a9194b1fb2df448614cd7e6a",
      "22af672dfbae4de489cb465d0b76aa7b",
      "94adf882f59c41a3a6cdf18a26693b9e",
      "02ea187b531a42c8bc02dfb032784841",
      "8844ac472afe47058b50c91f2e07d940",
      "b0230ce9823542c4891b851bce6a0d83",
      "44fe05f704ce4262935450b4b1835ac6",
      "9f8fe02deb904e839098f919edf2474d",
      "6e20ab939ea84815b228b3aba95096f0",
      "4508ff22b858458f86be11d4e44c7717",
      "b034ad039d4d42a18b35ad2b0ed1a4c9",
      "903b7da2712149999aea0975eb58d579",
      "f314b2a638414af080272bec2a293c07",
      "fbb4919dbc8041d9b99a386be04887bb",
      "44ad041bd1474010aa217cb34fa14764",
      "e9ce147114c840eb8f7e7a222941d526",
      "028649935de84c2da7a5faa2c527782a",
      "3f7560a846f647fb8ef1a82a73e7a57f",
      "f6349c8078474fa8bbd44309fc89c588",
      "6eb70894cdf74c35a7a57fed176955ab",
      "453e83088ef44bf2b83244e092ee8568",
      "8648c41367a74f6f9d66d577453e832f",
      "6ca6c2b6f9be4545ac06ba803cd6aa33",
      "ae07885b8c8b47f5a3b529150ffd060e",
      "599f8efadc9340f2848e42af7585a275",
      "e3cfe2aa89654b10bbde117e59580482",
      "b75cbc930102460d8f7c8d08fd7828bc",
      "47d9b4c088914baf92a318f821be8dd0",
      "462203a3009d4c90be60cb2575ff22bc",
      "0bab13d8c1584f369b0ab3f47bb67c28",
      "2a25a93785cb4428aafa15000f8865c0",
      "ad8a33e2fc5c42548497fafc6f4e2972",
      "af9d101fbc4645268a85379ec52eaf8a",
      "cfa3b57fbba448b888b7fd828dbbd1dd",
      "6bfbcb6697eb4cb3a47fb198b911ddf8",
      "6e517a6fb6bc4a56a90b4bf741f987cc",
      "8b92348b00dd43818c3a4e6a3c09ef1a",
      "615a7e3b649d42b2873c16b056573cef",
      "4e97af43be7f48a88b126c5e0a5ac969",
      "f838b76f9ad248cba3ba42677f2386c0",
      "bf9607d401eb4e7e9641bf7f5f86d483",
      "9639978b5577444cb868a9db0284809f",
      "9c700a30f6d447eda95d286d98874e8b",
      "dd75fba3dc934b20955fb5d520bc2512",
      "2e82cf7a123840eda76961f925c41155",
      "e8e301cf5bbb42f58176288e1d67ad79",
      "d97b64335a264a2cbd52841835b1aae8",
      "f291c8b344464dcda1558a2fdc7a34d6",
      "58fb8c21ce9c467cb1134156937a77e8",
      "f14b91ce42ff422eadfb4d34c3705a3a",
      "c833e5671eea4e80a937968ddcf99a7b",
      "098c115619cd4e97b33aa3442d421418",
      "78b59a4d971a470f8bbd9284a8531497",
      "0babbdb24ee1445e9ce47d8d59176bec",
      "a7890bd363c94f079f2243d6b96613b6",
      "aa871a325c0540daa7594ef772e4dcd3",
      "2b66b6dde4d540ef8a628f1d8d65833c",
      "7bc98b0cb7504bbc85bff87326fad1ed",
      "08e365f025b04b97883cb21cbe61a4e2",
      "3186545af18c4f43bb27541a1bcba41e",
      "8dc570f11cf44ea7aefdead7a8334703",
      "4700615b3d4544469acb4af1ca8f2fd1",
      "a284c26f721e4041ad4b0775eaaa79b2",
      "0784912cdc454b9c8ab02a7befe67f3b",
      "cb0d0359b1954f08b5492630085b54d5",
      "5b3459ec02864b24a99a2f77bc36918d",
      "fd94f4c3c5ed4adda3a5064625b3201e",
      "f89c8e4beb29423aace23c101060cd14",
      "d3cabc246d1e455f95600076f72b6eb7",
      "c0d08ccfc0244c4a845135f640ad09a8",
      "b549f5f43829480db402a555f87d229e",
      "e989c074ea284f108c7b748647291a39",
      "a324e82f691a4e1499a7a6590cc61e7d",
      "5e494913f3344208a7a7615ca24174c2",
      "04ac0becbf024c8f900e303e7135d5a3",
      "d9b3049e7d9c4acebccd0330d72ef3b7",
      "cb95e9e2a5cb462abfb1d99987814962",
      "9b5305c1811e4ed4a4b4b89071b3f044",
      "9fc165129dae43d4a38e145c821ddcc8",
      "d9b5bc8a04db4f398c32d60c72ebf688",
      "75b42a75375f4860b4888df1cdd99b5a",
      "d601240db4574347b7fdbc8f2c93edc4",
      "71c24a5792b748eaae05641579e40479",
      "f23b0b70788f47068a4cf9f75e0469f1",
      "fe4f4c248d9647a09716efda129e165b",
      "f98d14bd960a4a6e907c165b122885b9",
      "59edf71a1c92405fb520bf59b9bb00ff",
      "b55eb19721384e7aa6371f3674d6e9ef",
      "417ab2892b3e48cab13efa53f1cd6b48",
      "aa06007836d54de3bc842ddf5653a943",
      "e86451ec3f3f4ef89aa765ea45cd6151",
      "fa07a3ad67dd4099bd81c9776c2c3e9c",
      "d101ae4d6c2f496081c8fcce10a4a3d1",
      "02d6472116ea404294a7563da74c75ef",
      "48d6c0e396004ba590a1d9cf98c66715",
      "05c89c50595b4b57afd1f11e5a8bb958",
      "f96ff32d3bda4d508490f1a3896a2448",
      "a631c47a45aa4bc4a6b3843458673b62",
      "ebba64d5ef19431ea94f6935e49e6fca",
      "8aafda6c38b54a0cb4feb56f094cb477",
      "41952efb38494b9b98b7e058c7304175",
      "8ddfd2d82d5e4f5ebca76ab356ec9b1b",
      "f33831becf324531a761bc28985c3dce",
      "340a27fe567b439ea0e2adcc44dc5e02",
      "1df3f19b21f744709d63f6b8c5d0291f",
      "1a67ca71e7ec4504abfbbc22f9546548",
      "07dbd6078dae47d18a9aae3d4feb3fd7",
      "638b385c900e4b2386e20a3cd82a7710",
      "5cd7c55f450440c89695cc3980003c02",
      "81129572b3204ce5b00e150adbad0ef8",
      "defa57e71d454b019b8a42ca15ea0ff1",
      "d949ec206ad846b98bd65645a94f3add",
      "1dfc3522b92645f2be264b6c25e4d37a",
      "4a5c96fb7be44aeeb7cde0770bad6d06",
      "b912d4202e69489e959bdb7b36cb055f",
      "eace714339c34617bb8d8bd74e1e3449",
      "3a3c905bafc043abb371196cc3fdf17b",
      "a6d679814b7545eca04eff3fc75510ef",
      "d6edb1b074684e53ab8574038cc24969",
      "e63eba97eaa94a499de14f57aa276b62",
      "edc64d64a52e4182b6503c50be2f7eed",
      "a6894e913237495b8ac78dc0917f3e10",
      "b95f2d50564a4339b89b4dee134fdae5",
      "ea36999f25f541f6b7220ec5ca227d60",
      "fcc3e88013ff4472aa0972f7eeb4117a",
      "1b0a391bcb8545a2a4a5977e680f38cf",
      "32f16dae8e5b4dd6b81c892024fab5a0",
      "b67385a8291146778ec56fc3395cf116",
      "91cca6aeb3da4be7ae30cf120793312f",
      "8e6cfb2955bf4dc9b6cb41bcea715ba2",
      "d3fd558e0bc0448b950da489aa100d2e",
      "973c365e5677434bbb40b2acf036c712",
      "77bb7b389fdc41869403d3960b997acd",
      "acae2388b2fa4bc3b4f601eb5d38abc8",
      "357b20711b6b44c59d15dd6055fb80e4",
      "924f398380f74fa0b15edf64041c62e6",
      "c32ff004ee444e97ba85e8cd32ebba40",
      "11722822982e4aa9837661ebb4067f90",
      "e46491ec2c8e47c185cdb654ddc572a5",
      "b51c1bbfb6da44cdb118d38932d43ab8",
      "489e6940f4104843b666916452c102d2",
      "20003335aa964258ac3d4d5105a47f2a",
      "b4198590d1394a26a4f3e455f0f4df2b",
      "e649bc67ff704b658cc97aa2c837ab7e",
      "55c719290ca641ea8b0ba029902e0ab1",
      "ec697f7b4e98426a9284f96aefdd7075",
      "0579064d693546b087718e977cbaf880",
      "edff806125834c34a27936b8d07f82af",
      "ee8a098154e94db38e1f805620694e85",
      "4d33878c30a843a0ad510e494412546e",
      "8e1e312c48b648e591f5a4a764d2e41e",
      "9cd089e610934b11abc2a42ca9cc5d72",
      "a09a281011ad46cb9d116efb3ffe27c0",
      "ea994c3ab6e44208b3006704e3a4ed1c",
      "95592b22b60a46f1a95cb942c283225b",
      "cea4f18e35544a0b92aff2d6ad7b5565",
      "19ae1a2ad61d4f419c613f3841dbdb18",
      "746ec8f273c14b6ca184c5b01b81d6e7",
      "2a148a9d5d8940c1a9d4f00417514dcb",
      "09949b2c672749459eacd57e3ac5ca6c",
      "317d2d3edbae4ed1b152906f6833e74b",
      "9ec0801560a349cc818b757b32118541",
      "8ee64ac1734a4dd096687d3585f5b022",
      "3b7c273f8cda4564aec06507c83168bb",
      "0369555acf224ade965d8a386c529f8f",
      "6d5b489051864c28abbc3cad88e9c254",
      "b174259ba63a4c4cae93d4e5adcc3636",
      "ecc26775ac5f416897dd9b8631626f55",
      "e8f3735aafd14226bdd3c9d94333513a",
      "1fc213713ead4e12b2a9cf55eaa3be68",
      "9b70011650ae4d2b890a961cecfe1bb1",
      "175ed73fb9a848ce88fabfd362b13cbf",
      "9adcbe6f3b8849c9b09a8b09fc9b37b7",
      "04550aaba5a94ac5af43b9e0139127df",
      "cea0e15fb0fd41038e9ea3931e363534",
      "863961e950f9426e96be8d2d0bd17226",
      "1fc86b6dc5ef4acab4384675204d3260",
      "64570369be1c42adbac008588c57d5db",
      "79d9c5de608a461abbf1c99bf3f90fcc",
      "5103bd2b30f149cc8ce1c4e6b0a3c797",
      "d0f0d3b3fc33476aacf7a6c4a811590d",
      "628bec613f0347a896070cf0b2b570a2",
      "a8af9ebaa11a4de3af728c0b621d1f1b",
      "7a389d1526b04ff4baaef5ae3e6d9e13",
      "59cb14c6cda44379ad165324d988ec57",
      "5777cdcff01047ecaf3aae18b10afcc8",
      "6c3df46fc30d40ff95acff57080653ed",
      "a68896ea27674c5c9bd66415c83f174d",
      "9faa14a652db47919bbbe6fe4b3a7c4b",
      "7dde82bf6a3046a19b5669881bef0933",
      "be1548ed245443fca11f9837b8073dc9",
      "c02ea7b80fa641fbbdc392851b47f600",
      "edda385997f143dc9996745141d7b680",
      "9dce43f901d347b98d86f8d3aa52e049",
      "59da7721374849cd9bd9a15954f5dcc2",
      "1adef789a9114124ae0d5f6553eb1c54",
      "9537bc9256a44e6db50f3a642f6784de",
      "a75a08864ccd41e58dceb92159e8c20e",
      "52ef7d6a827f42e1b4da1292524549f7",
      "86047e4a50b140959e454761c3daf55b",
      "4dd44ed914504c81a86222d02c74e632",
      "5bb44ab3a24141b68035cc63d90743c6",
      "786e557a229d4cc09de73730793f4494",
      "a8a14c993dd74810ba386fc8203ea773",
      "f034676e1a0044cbad85f2ce45c54d6f",
      "ef9799febec84c3f9400c2769f4dbcd8",
      "8c1224ab08ac4bcfb00ccc9c94322415",
      "06394ca728c84643b48944061ef5b238",
      "c326f24ed375442e97cb76faa689d92e",
      "a9a199aeda0d480482adc356db15cd4e",
      "53fdeb7730744771b598132f6ccb81b1",
      "26f60efeb4ad4f6e91afc00355b1bf46",
      "ac508d4effa54714b5e028316acdb631",
      "9b677bf849214df3a6980ecee032217c",
      "9a4787bd9def41a8a23dbf28f717f9dd",
      "dc90b1f3fee44cc988e64e1a014f23d1",
      "e79447bf8301448e8e931ab354f87344",
      "1daa8a755bf24fdb8d882f58700316d6",
      "f9cb448a649c410faffcfda18b3ccc18",
      "2d53773ebdc94280af533e38b685adc4",
      "89d45effa7d54b65b752d42506ba8eea",
      "2e4bd6d9e27e40d19aed9155ccdff758",
      "62be387bd64d42da89ccbffd047d680e",
      "beb22e1fcb174f91ae3a94a59c5dae8d"
     ]
    },
    "id": "1qcl8bhYVQj9",
    "outputId": "df3cd772-caae-4b13-9691-d82c0a06c0a6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036613db532c4ac19bc597a930cfd0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff687bfaa4be49e0a2fabb62fd023db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8844ac472afe47058b50c91f2e07d940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00002.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ce147114c840eb8f7e7a222941d526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00002.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75cbc930102460d8f7c8d08fd7828bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615a7e3b649d42b2873c16b056573cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fb8c21ce9c467cb1134156937a77e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3186545af18c4f43bb27541a1bcba41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b549f5f43829480db402a555f87d229e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d601240db4574347b7fdbc8f2c93edc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d101ae4d6c2f496081c8fcce10a4a3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340a27fe567b439ea0e2adcc44dc5e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b912d4202e69489e959bdb7b36cb055f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0a391bcb8545a2a4a5977e680f38cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32ff004ee444e97ba85e8cd32ebba40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilgpt2: 100%|██████████| 2000/2000 [11:05<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilgpt2 BLEU: 0.20\n",
      "Final Reference Sentence: ling Lesnar 's career entitled Brock Lesnar : Here Comes the Pain . It was re @-@ released in 2012 as a three @-@ disc DVD and two @-@ disc Blu @-@ ray collector 's edition to tie\n",
      "Final Generated Sentence: ling the history of WWE Home Video.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gpt2: 100%|██████████| 2000/2000 [14:39<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 BLEU: 0.92\n",
      "Final Reference Sentence: ling Lesnar 's career entitled Brock Lesnar : Here Comes the Pain . It was re @-@ released in 2012 as a three @-@ disc DVD and two @-@ disc Blu @-@ ray collector 's edition to tie\n",
      "Final Generated Sentence: ling the WWE World Heavyweight Championship match between Randy Orton and Randy Orton Jr.\n",
      "\n",
      ", WWE Home Video released a DVD chronicling the WWE World Heavyweight Championship match between Randy Orton and Randy Orton Jr. In 2004 , WWE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edff806125834c34a27936b8d07f82af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a148a9d5d8940c1a9d4f00417514dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc213713ead4e12b2a9cf55eaa3be68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f0d3b3fc33476aacf7a6c4a811590d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02ea7b80fa641fbbdc392851b47f600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786e557a229d4cc09de73730793f4494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b677bf849214df3a6980ecee032217c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai-community/gpt2-medium: 100%|██████████| 2000/2000 [39:02<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai-community/gpt2-medium BLEU: 1.23\n",
      "Final Reference Sentence: ling Lesnar 's career entitled Brock Lesnar : Here Comes the Pain . It was re @-@ released in 2012 as a three @-@ disc DVD and two @-@ disc Blu @-@ ray collector 's edition to tie\n",
      "Final Generated Sentence: ling the events leading up to WrestleMania XXVIII. The DVD features interviews with Vince McMahon, Vince McMahon Jr., and Vince McMahon Jr.'s brother, Vince McMahon.\n",
      "\n",
      ", WWE Home Video released a DVD chronicling the events leading up\n",
      "\n",
      "Final Results:\n",
      "distilgpt2: 0.20 BLEU\n",
      "gpt2: 0.92 BLEU\n",
      "openai-community/gpt2-medium: 1.23 BLEU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_kbeams(models: list, num_examples: int = 4000, prompt_length: int = 10, max_gen_length: int = 50):\n",
    "    \"\"\"Evaluate custom k-beams search on multiple models using WikiText-103.\"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='test')\n",
    "    results = {}\n",
    "\n",
    "    for model_name in models:\n",
    "        # Load model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name).to(torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "        # Initialize your custom beam search\n",
    "        generator = CustomKBeamSearch(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            k=5,\n",
    "            initial_window_size=3,\n",
    "            max_length=max_gen_length\n",
    "        )\n",
    "\n",
    "        # Generate hypotheses and collect references\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "\n",
    "        for example in tqdm(dataset.select(range(num_examples)), desc=f\"Evaluating {model_name}\"):\n",
    "            text = example['text'].strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            # Tokenize and split into prompt/reference\n",
    "            tokens = tokenizer.encode(text, return_tensors='pt')[0]\n",
    "            if len(tokens) < prompt_length + max_gen_length:\n",
    "                continue\n",
    "\n",
    "            prompt_tokens = tokens[:prompt_length]\n",
    "            reference_tokens = tokens[prompt_length:prompt_length + max_gen_length]\n",
    "\n",
    "            # Generate continuation\n",
    "            generated = generator.generate(prompt_tokens.unsqueeze(0).to(model.device))\n",
    "            generated_continuation = generated[len(prompt_tokens):]  # Remove prompt\n",
    "\n",
    "            # Decode texts\n",
    "            reference_text = tokenizer.decode(reference_tokens, skip_special_tokens=True)\n",
    "            generated_text = tokenizer.decode(generated_continuation, skip_special_tokens=True)\n",
    "\n",
    "            references.append(reference_text)\n",
    "            hypotheses.append(generated_text)\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        bleu_score = corpus_bleu(hypotheses, [references], force=True)\n",
    "        results[model_name] = bleu_score.score\n",
    "        print(f\"{model_name} BLEU: {bleu_score.score:.2f}\")\n",
    "        print(f\"Final Reference Sentence: {reference_text}\")\n",
    "        print(f\"Final Generated Sentence: {generated_text}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "models_to_test = [\n",
    "    'distilgpt2',\n",
    "    'gpt2',\n",
    "    'openai-community/gpt2-medium'\n",
    "]\n",
    "\n",
    "evaluation_results = evaluate_kbeams(\n",
    "    models=models_to_test,\n",
    "    num_examples=2000,\n",
    "    prompt_length=10,\n",
    "    max_gen_length=50\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "for model, score in evaluation_results.items():\n",
    "    print(f\"{model}: {score:.2f} BLEU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2e8QNiOV9s3"
   },
   "source": [
    "## Rouge Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ThB_ryXLoTkb",
    "outputId": "6870406f-881d-49a8-b987-c4ddea66502d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilgpt2: 100%|██████████| 2000/2000 [10:58<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Reference Sentence: ling Lesnar 's career entitled Brock Lesnar : Here Comes the Pain . It was re @-@ released in 2012 as a three @-@ disc DVD and two @-@ disc Blu @-@ ray collector 's edition to tie\n",
      "Final Generated Sentence: ling the history of WWE Home Video.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gpt2: 100%|██████████| 2000/2000 [15:20<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Reference Sentence: ling Lesnar 's career entitled Brock Lesnar : Here Comes the Pain . It was re @-@ released in 2012 as a three @-@ disc DVD and two @-@ disc Blu @-@ ray collector 's edition to tie\n",
      "Final Generated Sentence: ling the WWE World Heavyweight Championship match between Randy Orton and Randy Orton Jr.\n",
      "\n",
      ", WWE Home Video released a DVD chronicling the WWE World Heavyweight Championship match between Randy Orton and Randy Orton Jr. In 2004 , WWE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai-community/gpt2-medium: 100%|██████████| 2000/2000 [41:07<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Reference Sentence: ling Lesnar 's career entitled Brock Lesnar : Here Comes the Pain . It was re @-@ released in 2012 as a three @-@ disc DVD and two @-@ disc Blu @-@ ray collector 's edition to tie\n",
      "Final Generated Sentence: ling the events leading up to WrestleMania XXVIII. The DVD features interviews with Vince McMahon, Vince McMahon Jr., and Vince McMahon Jr.'s brother, Vince McMahon.\n",
      "\n",
      ", WWE Home Video released a DVD chronicling the events leading up\n",
      "\n",
      "Final Results:\n",
      "distilgpt2: 0.07 BLEU\n",
      "gpt2: 0.10 BLEU\n",
      "openai-community/gpt2-medium: 0.11 BLEU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_kbeams(models: list, num_examples: int = 4000, prompt_length: int = 10, max_gen_length: int = 50):\n",
    "    \"\"\"Evaluate custom k-beams search on multiple models using WikiText-103.\"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='test')\n",
    "    results = {}\n",
    "    scores = []\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    for model_name in models:\n",
    "        # Load model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name).to(torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "        # Initialize your custom beam search\n",
    "        generator = CustomKBeamSearch(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            k=5,\n",
    "            initial_window_size=3,\n",
    "            max_length=max_gen_length\n",
    "        )\n",
    "\n",
    "        # Generate hypotheses and collect references\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "\n",
    "        for example in tqdm(dataset.select(range(num_examples)), desc=f\"Evaluating {model_name}\"):\n",
    "            text = example['text'].strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            # Tokenize and split into prompt/reference\n",
    "            tokens = tokenizer.encode(text, return_tensors='pt')[0]\n",
    "            if len(tokens) < prompt_length + max_gen_length:\n",
    "                continue\n",
    "\n",
    "            prompt_tokens = tokens[:prompt_length]\n",
    "            reference_tokens = tokens[prompt_length:prompt_length + max_gen_length]\n",
    "\n",
    "            # Generate continuation\n",
    "            generated = generator.generate(prompt_tokens.unsqueeze(0).to(model.device))\n",
    "            generated_continuation = generated[len(prompt_tokens):]  # Remove prompt\n",
    "\n",
    "            # Decode texts\n",
    "            reference_text = tokenizer.decode(reference_tokens, skip_special_tokens=True)\n",
    "            generated_text = tokenizer.decode(generated_continuation, skip_special_tokens=True)\n",
    "\n",
    "            references.append(reference_text)\n",
    "            hypotheses.append(generated_text)\n",
    "            scores.append(scorer.score(reference_text, generated_text)['rougeL'])\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        # bleu_score = corpus_bleu(hypotheses, [references], force=True)\n",
    "        results[model_name] = np.mean([s.fmeasure for s in scores])\n",
    "        print(f\"Final Reference Sentence: {reference_text}\")\n",
    "        print(f\"Final Generated Sentence: {generated_text}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "models_to_test = [\n",
    "    'distilgpt2',\n",
    "    'gpt2',\n",
    "    'openai-community/gpt2-medium'\n",
    "]\n",
    "\n",
    "evaluation_results = evaluate_kbeams(\n",
    "    models=models_to_test,\n",
    "    num_examples=2000,\n",
    "    prompt_length=10,\n",
    "    max_gen_length=50\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "for model, score in evaluation_results.items():\n",
    "    print(f\"{model}: {score:.2f} BLEU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7T58QiCV6Sz"
   },
   "source": [
    "## BertScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b7dae88eab784a7e89090a3c8f5efeff",
      "3f92faa3f96747148d7937ee77b2e278",
      "34b0ef2f58c547fea2ee1fe2356ae39f",
      "a80ac460d96b49beab9e0f0c18576997",
      "1057ad737f10461390564d33bb3f3291",
      "928c30e027e84bf9b0899547dd36f6b0",
      "8503e3f2611642719e6382373042a049",
      "237b6c55c5d44039930d6840ae57b813",
      "9025a01bb8b04209a4fbf749063f240b",
      "84091c1ca18c4542bd3733096ba53f95",
      "eb736b4c45dd4413ab311fff46257d43",
      "7c23726c25fd4d63b52193da4348c78e",
      "16aa7d16d723474ea06695dd74b976dc",
      "224b1e3a305f4934b940eaa3ba04ab6d",
      "70ad4356517e415ca958a44097e008f8",
      "e964be1496fc48c7add4ff33e4cd3efc",
      "2629912764dc4e119d1dff8412d9c44e",
      "659411a893354d478d465c156c39eabb",
      "6ea24ac597dd4e7ca76d109c6cf5aa7b",
      "f766c16d3e5541caa80318ff29c97acb",
      "e125c841d7bb4633bc1feed32f296708",
      "728a7ebe85fb4918a4e1fd5a6d5c66a9",
      "14a8b5292b6a447e9f2a42399b24b8f5",
      "fa19638153a14ce49555d9de0434e367",
      "a6f0d88b00e44025be3b0c68f5ff31e7",
      "443f1af90adb43fe9f72bd3e85bce498",
      "232d279c5bfa444ba759b1fd6944a7a3",
      "f4897ae9276647d8a5d5e755c16e9a1b",
      "4bdfe5e0fdec437c98de011db22aa7d0",
      "127c7ff91f464388b25fff934a25b75b",
      "6cd07acf8fe8431eb67b9ec497c2701d",
      "07ebd9199bd346649989495794f1549d",
      "a6547202e8034b1e89cba347d1fbe191",
      "3a0154e7e5ab4cee88628645b2b32c94",
      "da3c5670e8164ad7a8f14ab2cb6bf22d",
      "a3e1f28315f24a04933679825287cf86",
      "ca309edcfabc4ad183da0f140c2bfbe2",
      "b9633a49689e41329b36765c39de64df",
      "e40d769d64764a14ac733689c96b4293",
      "bded817972f346229c1697dee6431772",
      "0f6972a44bd948a6bfdf7a32531c5c51",
      "0f186b5d29c74a30a05141207d04c70c",
      "54aec4ca925f44e8bbc146f69676c5c6",
      "ea897a74c3984441aff6a60aa5bc64be",
      "24a185d193e2434e964bc26e31f0f283",
      "e7b90f7927ee45db922d40a0ebf9da85",
      "ce29672c81ef43f290a1fc10a42a360f",
      "f26f7860490741e7bda2ac59ef1f12ea",
      "08d246820b334f0a84ed808d683e53d3",
      "3b305a0a7cd241c1ab076fbd5ca51a93",
      "618878d07eb94273ad5cccb67576b3f5",
      "ba88e90e56bd42cd9334bb674f681781",
      "760c912d8932407b931949feb6ea6875",
      "9c6a5854f5bb4954baa4327d5eb0c5fb",
      "27105614d93f4dbabfd2e9b5b0eece45",
      "3ca54f86cb65496faf3c71434da8764d",
      "ec6f79be977544bd87d0541a05618ca8",
      "b4e7d88327b04a9194d996edcedd2b61",
      "cbaa3b8a757e42819652894964dd009d",
      "063395bea5b1460f8565d3792f14861c",
      "e146d4a9033340b6820339910c23434e",
      "0f9329f30b2f4670bf81559463713bc4",
      "bcd69531c4e34f81ae3e94505fae86ee",
      "0febdd5eb80e44cda53a1f236082217a",
      "7ba0020983f54362bf3e45673f217082",
      "d71210085ff1457bbeac657846bc5ec4",
      "2415d8cfd84643e39090c656e9e9a117",
      "1ebea0db51ab408e97d4ad6b7f1be19a",
      "759be3ad46d9470e8730379e2d5d51ef",
      "3f5185000ced46d8b5612ab34e52a173",
      "50d2d23cb1e84d7ea77136f01b3092fd",
      "962aad5118054ae8940a048f10ac02b2",
      "03e5423c67e942938d19cb59aee2a38a",
      "a47f6666d69d449c8ae01674c63998cd",
      "e58458871fa74df8a619b61429e16a35",
      "199f00c8e4974d5f986cda7190d85986",
      "8f52eb49f1644350ac09686a3806bfde",
      "3cfead50f53f49ed9facd38a08a6517d",
      "63d5df6a63fb475eb54ec77da1f78269",
      "7af95288be36463eb13f1d00f52ef33f",
      "b73bd4f9522f429bac5ae4581600c6b1",
      "dd1ce8ce607049fe88eab13c1b549105",
      "e5d583098d3d47fba052ba85a79c8da8",
      "aefef20e3596495599aabe89ad69d617",
      "1c6a1f663e2a48a4800f4048463df5a0",
      "8c2c91bdcb51440d9ec167a6230d4f8a",
      "73727f56723742179deb968473044982",
      "262a8114a1544b3aab5149366e176b40",
      "471c5bdb470d4b83a3994da7615c229b",
      "db0060ba2e9c4fc89950980287ce11f7",
      "b73342b7fa7843428ac6ff0472339023",
      "2fc01bd908fd411a80fb50a8399b4381",
      "603209433abf4255b6d9c97234749863",
      "c3e810535bc044ef91c428a7eda0e96d",
      "32caaceff05445298ee5c83ccfa3488e",
      "ed7041864b5746b2938cc1d700068489",
      "a8020d4c3f1a4731b273e398553e2a7f",
      "4eb34c75a50e45a4a25cdd232303e01c",
      "646f8659364a452a9fdaca61f2c11738",
      "8ddd8d03bb89486ea6f4c8bb3b31899c",
      "8e00709f450840caa472c474cebe01dd",
      "6e93f22c37134be5b645c1d6c45d9c93",
      "2545117da1544ea59a09e744c34a1f41",
      "2137d138109d4ded858e4523522641d1",
      "6369595a82174e8ba6691dd74a4b82e2",
      "a19e3e452439481ea704ad228b944c88",
      "094e18d66dbe499088a37fc04e848c17",
      "20c5bfcd848c4c4ba6484fcbda4068ac",
      "8bd7004f042a44ceb622625e2ac01f36",
      "9fd7cd29a3574545b2e44ce56378f79f",
      "d70474f626e44b10b290cdabe4607d58",
      "2f60add58373428e97e80334b2fcd142",
      "55aeb129cddb475a9a0b3a55418958ce",
      "be7a2df3f9ff4dbab2413eb9bf3fe566",
      "63d4b765271b4d4b8dbaff21f08a8314",
      "d4076536940747aa9dea0aa7863e100e",
      "d29918fbc4b24e30877969ab68ec86c8",
      "61ce76e9729741d19ee7eff8f1e2512a",
      "0060ec19ce3d4ac88c881eb000500372",
      "634fdd1cc30745b09f3fd952dd279d2a",
      "fe93257cb73543e195818ed6a97a74c2",
      "67510f2588a44c75a557ad67285ffe67",
      "0c9ea5ea99fe4f1fad916a134f0785c4",
      "f8de363d2b024b13a5dba916c2f16507",
      "4efadabf26a249fc86040287dcc4aaf7",
      "75c3378955464531bcc77ab24188e691",
      "201b503d7d2c4bcfbfbee82f7eabc2d6",
      "e4c4256492a743f4b5d6cffa6411d07b",
      "e15781c5415348188bc595c7322e9e13",
      "ae32cb29144847dc980336e5a34d3577",
      "d333db50f2d846698bd6a16068bbbd7f",
      "07da640c085a4812bb8c8dae60346c0c",
      "a9a5372b2b8d426c90c9ab88570ba49d",
      "de0536678371479c92d8cce8f5b3ded9",
      "98726a6883cc4671b09cf43e9744511d",
      "6bf577364ad5490aa19b3db8284582ac",
      "b287b63abb054f0db36d5de63010df11",
      "640c486a58354b508a4a8f9274bcb8a0",
      "43cff75a2c8840fcb8e3d9da938c5baa",
      "580ba7de6f6c458eb08596ad2197327b",
      "4ce76116d471472e948f14df21ecb89a",
      "b2b3aa99422c4f01a71bd48ac772621c",
      "b8411a8f51384c46a339cceca0140c22"
     ]
    },
    "id": "eVpgOQNlpUan",
    "outputId": "9f34f68c-1600-4074-cc7f-8933b60868e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilgpt2: 100%|██████████| 2000/2000 [11:00<00:00,  3.03it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7dae88eab784a7e89090a3c8f5efeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c23726c25fd4d63b52193da4348c78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a8b5292b6a447e9f2a42399b24b8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0154e7e5ab4cee88628645b2b32c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a185d193e2434e964bc26e31f0f283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca54f86cb65496faf3c71434da8764d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Reference Sentence: ling Lesnar 's career entitled Brock Lesnar : Here Comes the Pain . It was re @-@ released in 2012 as a three @-@ disc DVD and two @-@ disc Blu @-@ ray collector 's edition to tie\n",
      "Final Generated Sentence: ling the history of WWE Home Video.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gpt2: 100%|██████████| 2000/2000 [16:52<00:00,  1.98it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Reference Sentence: ling Lesnar 's career entitled Brock Lesnar : Here Comes the Pain . It was re @-@ released in 2012 as a three @-@ disc DVD and two @-@ disc Blu @-@ ray collector 's edition to tie\n",
      "Final Generated Sentence: ling the WWE World Heavyweight Championship match between Randy Orton and Randy Orton Jr.\n",
      "\n",
      ", WWE Home Video released a DVD chronicling the WWE World Heavyweight Championship match between Randy Orton and Randy Orton Jr. In 2004 , WWE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2415d8cfd84643e39090c656e9e9a117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfead50f53f49ed9facd38a08a6517d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471c5bdb470d4b83a3994da7615c229b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddd8d03bb89486ea6f4c8bb3b31899c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70474f626e44b10b290cdabe4607d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67510f2588a44c75a557ad67285ffe67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a5372b2b8d426c90c9ab88570ba49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai-community/gpt2-medium: 100%|██████████| 2000/2000 [42:08<00:00,  1.26s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Reference Sentence: ling Lesnar 's career entitled Brock Lesnar : Here Comes the Pain . It was re @-@ released in 2012 as a three @-@ disc DVD and two @-@ disc Blu @-@ ray collector 's edition to tie\n",
      "Final Generated Sentence: ling the events leading up to WrestleMania XXVIII. The DVD features interviews with Vince McMahon, Vince McMahon Jr., and Vince McMahon Jr.'s brother, Vince McMahon.\n",
      "\n",
      ", WWE Home Video released a DVD chronicling the events leading up\n",
      "\n",
      "Final Results:\n",
      "distilgpt2: 0.79 BertScore\n",
      "gpt2: 0.80 BertScore\n",
      "openai-community/gpt2-medium: 0.81 BertScore\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from bert_score import score as bert_score\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_kbeams(models: list, num_examples: int = 4000, prompt_length: int = 10, max_gen_length: int = 50):\n",
    "    \"\"\"Evaluate custom k-beams search on multiple models using WikiText-103.\"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='test')\n",
    "    results = {}\n",
    "\n",
    "    for model_name in models:\n",
    "        # Load model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name).to(torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "        # Initialize your custom beam search\n",
    "        generator = CustomKBeamSearch(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            k=5,\n",
    "            initial_window_size=3,\n",
    "            max_length=max_gen_length\n",
    "        )\n",
    "\n",
    "        # Generate hypotheses and collect references\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "\n",
    "        for example in tqdm(dataset.select(range(num_examples)), desc=f\"Evaluating {model_name}\"):\n",
    "            text = example['text'].strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            # Tokenize and split into prompt/reference\n",
    "            tokens = tokenizer.encode(text, return_tensors='pt')[0]\n",
    "            if len(tokens) < prompt_length + max_gen_length:\n",
    "                continue\n",
    "\n",
    "            prompt_tokens = tokens[:prompt_length]\n",
    "            reference_tokens = tokens[prompt_length:prompt_length + max_gen_length]\n",
    "\n",
    "            # Generate continuation\n",
    "            generated = generator.generate(prompt_tokens.unsqueeze(0).to(model.device))\n",
    "            generated_continuation = generated[len(prompt_tokens):]  # Remove prompt\n",
    "\n",
    "            # Decode texts\n",
    "            reference_text = tokenizer.decode(reference_tokens, skip_special_tokens=True)\n",
    "            generated_text = tokenizer.decode(generated_continuation, skip_special_tokens=True)\n",
    "\n",
    "            references.append(reference_text)\n",
    "            hypotheses.append(generated_text)\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        _, _, F1 = bert_score(hypotheses, references, lang=\"en\", model_type=\"roberta-large\")\n",
    "        results[model_name] = F1.mean().item()\n",
    "        print(f\"Final Reference Sentence: {reference_text}\")\n",
    "        print(f\"Final Generated Sentence: {generated_text}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "models_to_test = [\n",
    "    'distilgpt2',\n",
    "    'gpt2',\n",
    "    'openai-community/gpt2-medium'\n",
    "]\n",
    "\n",
    "evaluation_results = evaluate_kbeams(\n",
    "    models=models_to_test,\n",
    "    num_examples=2000,\n",
    "    prompt_length=10,\n",
    "    max_gen_length=50\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "for model, score in evaluation_results.items():\n",
    "    print(f\"{model}: {score:.2f} BertScore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-4q2YZ4V3qb"
   },
   "source": [
    "## Mauve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5956b81134d94d6cb1580b6baf6f61fd",
      "2ccfd9e629b34c46aafbd4cdd27abffe",
      "f3d41403e91d4b3f989d582bce5c59b0",
      "70f56a0ea8bd4e478718aeaaa46bf0ed",
      "f1b360a96fdf4b2f9fb3faf0e627976c",
      "e4cb9943d26e4d82a4624217a956c90a",
      "ea22585c470b45ac95446cd20aee430f",
      "3cffdaab8796496b97387ec44f000718",
      "bfacc448365d4d4fa81c1e7fbeb8e774",
      "374c5b8c05e04a529ba8e215e3fff5de",
      "86d379e3fca24174aa0ebdd835cb2214",
      "54cd4fa6ef3d4764902f2d2c0cbaeaec",
      "91f28676db7f479b8d616f7b80a9b07c",
      "dd1d4b4b156c4908bc2a3b138c4de873",
      "ae1f766109d6448b9ce065048c062d29",
      "73502cdfc35e4b84932aeaaf959f7fdd",
      "0c9204de9ff34d9886fe07e8d362e281",
      "d1b138afcc7b4cf28828504029e991f1",
      "d9041bb2fb184a5cbd979945b7c4de9d",
      "10d1838f3d3841daab75da268c577592",
      "2b219e9405034dd99408ef82cc7a459f",
      "232b9bc46bcd4269a5eb5a733ffc43c5",
      "31bc0d02d44842ada0dcd8030ac54c15",
      "7acbf701aa894335beeedecb8ef3eaeb",
      "c9958de0ec0e4569b6e79145b704e0a8",
      "7bf7388910e24ccda23c272a3d381616",
      "d2615a5874644d629fc734adc8627b33",
      "48d3e3a19b7841eaaf17d7cf25287322",
      "03235869661f42bab09b4eb295844bef",
      "d4d5ca66ce1a4f56bb57fabbd22e4a37",
      "ba83fd159255478f9539ed098f887bd2",
      "d4e787d400364cce89a07304e6e19584",
      "8a1ef2cba11343ac93fa16fe4d84dbde",
      "d4547a3b002c4f51a78f523b1acb4114",
      "b564c5eeaf8f48c58b6a0b0c562ed23a",
      "9cf4ba8167804964bd1024672ca51158",
      "7f18701640c24a6089c60d80b656e3ec",
      "c856449022c34f30ad923060cc6dee54",
      "48a64f94ad7f40fbbd3bb241867e780c",
      "7b4042fa67fc4bec96a301eb6cf6b6fc",
      "bbbd852027d74afc97ca129f3d3e7205",
      "df951b9d2ca44de8a61f97ee9b11b100",
      "cc46851cee75413ab5043b99bc5bcdb6",
      "2e1fdc8866ba4b6f891cfed622eba955",
      "66db7c58dd894b08928571a6c972f7c7",
      "c1367e923b814ca5999af80c3312574d",
      "dba2828e75b64386a2795ea570ea7c0c",
      "f5b8dd5de62d47cc965642c810111c4e",
      "0eac61a6aece434bbc6e7d4a68a5cecb",
      "2ebc29c6852c42ea9e5a2ddbee2d1c04",
      "a6479e5a24c44ac88d3c196c3112267b",
      "abed14277ee847469a1c933a3bb388ff",
      "a1ad7f6920b941e685ff9545afd0b116",
      "42277269762e432fbb845580c6b06c7c",
      "e00d3ec4bb874f2292804aa489d869b6",
      "6d0efc53f20f45ceb0e9ab35d91bce50",
      "b343fa7f43394de2a38c509abff569fe",
      "42847d1885a34348a356337b68b737d2",
      "0b33e6e10aae4ab893df04a1a079f976",
      "0c854765f0f94b9fa74c66817d6435d7",
      "c3da51865306460cb9b2c8024ee6a08f",
      "2ee8b61d6dd94f9c987d7e73c5924c5e",
      "1170b496b9a5440895b671ca1a652981",
      "ffee5d26d53645e2acc68f7698877398",
      "300be9e963aa415d84e466f9c6dc49b5",
      "061f2fe68c20436a8c81cd507c76ecd5",
      "ab2c17c34e3a424980d5864b55a9928d",
      "c500c2239882439fae3467cd63810659",
      "20161212ec7c4966886d4b0c8be19780",
      "09b4672e7bb646b8837a16957c64d680",
      "fec102547900495a8c2fd029a2a324e7",
      "c51f33d9c27f439b979567661fe69e68",
      "2dfbe7066273428d91c1c348b97bfbbb",
      "d0ec853a6639471389e11b585a01b28c",
      "57fefbbc0508445497fdc264657a0be5",
      "af21240ac7444d2895f0246a77373080",
      "dd6e376ac7fb4de69af1fad348e26947",
      "0155bd2a5a484e71a7f8a7094eaf3053",
      "c3cd2e2a4dbf4af3879da5d0b7896be0",
      "fb9e314041de4d2c8239ab85585ef2ac",
      "cdfd15f7a88a4bd18e7060073b33fa36",
      "32e81ed47c544d78b0f37ea5a12407a2",
      "bc09d70bf79245fea4e7cac73110997d",
      "f7cd4d42893e411096f839cb317b08d1",
      "2ea9de3f85934619b3f2c47b36e4d41b",
      "c99ca4c7b57a4146b0688022b5a0c3bf",
      "b276162a23e84bdcb457d4c07975fc6b",
      "4ad55c7d94a84f50a0c1887656a0acc5",
      "217749a2569a4b81a1675e9253581371",
      "eba22a0833c346b8848f138487bb3de0",
      "c4f637a272c24490bf3a5a27889a4504",
      "06b116b7a6b94ea4b4da91f5d2a11787",
      "b1766276ea1542dd8b5b59a2fdbda53d",
      "e69c0a9c93ad40ddbcb4e0a14a9efa44",
      "28cbcd58bacc4a26b18ce6accf2997d4",
      "41271c63be5f4883a66f540fc6193d98",
      "2b8138248a924b9186c8f150eff9111a",
      "ddd45b27c8bb4d1a9cea2e417b239d9a",
      "4f05e80530904fb190559fda4434ead3",
      "c8c4e9ae90b54d26a1f11ae2910ce681",
      "b64e205798284176bdc7905e6c5b65b0",
      "65ccf8d7d4734af0983ebbfea5a7fc98",
      "ccf8ad13d66d4bdc8827522b09d4a3fd",
      "62b5603ab48f484fa590b6c773c598a3",
      "029ba1997baa46d59f6ea6f8edc1caa6",
      "0a1a0b6be2734ddc8eed261a8b72ee86",
      "7217b703349b42c2be7abd438fe14c9f",
      "18ebab2916c6444db816e23c0cf463b0",
      "a00d4f8e5f70457f8e2b56cf1bebe35d",
      "593ebcde7e3147a9a23b11beca019809",
      "339268684c5e4097a87be10f0da2f926",
      "c5fe0a2e548e4513892593b5ed68c081",
      "44c171c579ae4efeac0f3c3d57cb3a3f",
      "62b5c2d140fa4be8bc39ed42ef3c1abd",
      "fee0908a76c1423b9cf9b893ec42c529",
      "15f2716e889e4c43b55600651ddd2010",
      "69b8d3b227bd4d17ac72a3a393ed1c2a",
      "4081d5d8826448f7a015ac8f44ba874f",
      "8724c65569c5440295895eac446a4761",
      "f7202ceb017d499083023e3de760fcc0",
      "968c0531b2624ff89dbfa98b678d50d2",
      "1ca5eb8a178d45d9a61f315b85f22c99",
      "30d5a74dc8794220bb6127863c792c4a",
      "7343b0a5dc2c4643957335d7be452691",
      "3d4c4fcbd20440cca9291c95cc1a960d",
      "2e2d5606a7a848a7b1dbfa65dc37ab2f",
      "e8f12802cc9c4ef4971ab0519d8c6679",
      "1bbd9fa4447544a49b5f76d1eef41423",
      "92d16634a4c44f939a2439f083e4a768",
      "6e240bcccde8416085e2d95f8cacda64",
      "ea9b2fd08ac94c309f22bd51ebaede20",
      "d69e26ace1ef440abe3afa46535c32e8",
      "bc5a1dc6a5aa4efdad35fd8530555618",
      "57d91d32ffcd4546a2c71156785762f6",
      "f67b1ac423d84dd2b27bdbad8c6ec65f",
      "50d1eded14b441b8b91948f251b015e4",
      "89bf77325299452b86fb4972c4d5eeb2",
      "4afd764a2cc848bebc924681962489e0",
      "c6b4c86b6c284cab930fd03769e6ec39",
      "004bddd331504d098a4a6364a136a7d6",
      "09fb738f00f34775b7fc0912f9c959e7",
      "505d6580e92e456e897b7a6b1344d0c2",
      "1ce21f4027e541c092348e4316686eb8"
     ]
    },
    "id": "GfnXKc2HPIAR",
    "outputId": "dabf9114-df76-4bf5-faa8-ad56eb53af14"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilgpt2: 100%|██████████| 2000/2000 [10:52<00:00,  3.06it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5956b81134d94d6cb1580b6baf6f61fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing p:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54cd4fa6ef3d4764902f2d2c0cbaeaec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing q:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Reference Sentence: ling Lesnar 's career entitled Brock Lesnar : Here Comes the Pain . It was re @-@ released in 2012 as a three @-@ disc DVD and two @-@ disc Blu @-@ ray collector 's edition to tie\n",
      "Final Generated Sentence: ling the history of WWE Home Video.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gpt2: 100%|██████████| 2000/2000 [15:33<00:00,  2.14it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bc0d02d44842ada0dcd8030ac54c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing p:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4547a3b002c4f51a78f523b1acb4114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing q:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Reference Sentence: ling Lesnar 's career entitled Brock Lesnar : Here Comes the Pain . It was re @-@ released in 2012 as a three @-@ disc DVD and two @-@ disc Blu @-@ ray collector 's edition to tie\n",
      "Final Generated Sentence: ling the WWE World Heavyweight Championship match between Randy Orton and Randy Orton Jr.\n",
      "\n",
      ", WWE Home Video released a DVD chronicling the WWE World Heavyweight Championship match between Randy Orton and Randy Orton Jr. In 2004 , WWE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66db7c58dd894b08928571a6c972f7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0efc53f20f45ceb0e9ab35d91bce50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2c17c34e3a424980d5864b55a9928d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0155bd2a5a484e71a7f8a7094eaf3053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217749a2569a4b81a1675e9253581371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c4e9ae90b54d26a1f11ae2910ce681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339268684c5e4097a87be10f0da2f926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai-community/gpt2-medium: 100%|██████████| 2000/2000 [41:28<00:00,  1.24s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca5eb8a178d45d9a61f315b85f22c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing p:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5a1dc6a5aa4efdad35fd8530555618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing q:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Reference Sentence: ling Lesnar 's career entitled Brock Lesnar : Here Comes the Pain . It was re @-@ released in 2012 as a three @-@ disc DVD and two @-@ disc Blu @-@ ray collector 's edition to tie\n",
      "Final Generated Sentence: ling the events leading up to WrestleMania XXVIII. The DVD features interviews with Vince McMahon, Vince McMahon Jr., and Vince McMahon Jr.'s brother, Vince McMahon.\n",
      "\n",
      ", WWE Home Video released a DVD chronicling the events leading up\n",
      "\n",
      "Final Results:\n",
      "distilgpt2: 0.01 Mauve\n",
      "gpt2: 0.13 Mauve\n",
      "openai-community/gpt2-medium: 0.28 Mauve\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import mauve\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_kbeams(models: list, num_examples: int = 4000, prompt_length: int = 10, max_gen_length: int = 50):\n",
    "    \"\"\"Evaluate custom k-beams search on multiple models using WikiText-103.\"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='test')\n",
    "    results = {}\n",
    "\n",
    "    for model_name in models:\n",
    "        # Load model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name).to(torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "        # Initialize your custom beam search\n",
    "        generator = CustomKBeamSearch(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            k=5,\n",
    "            initial_window_size=3,\n",
    "            max_length=max_gen_length\n",
    "        )\n",
    "\n",
    "        # Generate hypotheses and collect references\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "\n",
    "        for example in tqdm(dataset.select(range(num_examples)), desc=f\"Evaluating {model_name}\"):\n",
    "            text = example['text'].strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            # Tokenize and split into prompt/reference\n",
    "            tokens = tokenizer.encode(text, return_tensors='pt')[0]\n",
    "            if len(tokens) < prompt_length + max_gen_length:\n",
    "                continue\n",
    "\n",
    "            prompt_tokens = tokens[:prompt_length]\n",
    "            reference_tokens = tokens[prompt_length:prompt_length + max_gen_length]\n",
    "\n",
    "            # Generate continuation\n",
    "            generated = generator.generate(prompt_tokens.unsqueeze(0).to(model.device))\n",
    "            generated_continuation = generated[len(prompt_tokens):]  # Remove prompt\n",
    "\n",
    "            # Decode texts\n",
    "            reference_text = tokenizer.decode(reference_tokens, skip_special_tokens=True)\n",
    "            generated_text = tokenizer.decode(generated_continuation, skip_special_tokens=True)\n",
    "\n",
    "            references.append(reference_text)\n",
    "            hypotheses.append(generated_text)\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        score = mauve.compute_mauve(\n",
    "            p_text=hypotheses,\n",
    "            q_text=references,\n",
    "            device_id=0,\n",
    "            max_text_length=256,\n",
    "            verbose=False,\n",
    "            batch_size=16\n",
    "        ).mauve\n",
    "        results[model_name] = score\n",
    "        print(f\"Final Reference Sentence: {reference_text}\")\n",
    "        print(f\"Final Generated Sentence: {generated_text}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "models_to_test = [\n",
    "    'distilgpt2',\n",
    "    'gpt2',\n",
    "    'openai-community/gpt2-medium'\n",
    "]\n",
    "\n",
    "evaluation_results = evaluate_kbeams(\n",
    "    models=models_to_test,\n",
    "    num_examples=2000,\n",
    "    prompt_length=10,\n",
    "    max_gen_length=50\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "for model, score in evaluation_results.items():\n",
    "    print(f\"{model}: {score:.2f} Mauve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3O3d7kKoAku"
   },
   "source": [
    "# Original K-Beam Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trSgN-Z7WaIT"
   },
   "source": [
    "## Initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-O1SIMCVyG4",
    "outputId": "acbed17b-0b38-4cb4-c43e-12e20c627cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distilgpt2...\n",
      "\n",
      "Prompt: Once upon a time\n",
      "\n",
      "=== Standard Beam Search ===\n",
      "Loading distilgpt2...\n",
      "Output: Once upon a time when the world was in a state of flux, the world was in a state of flux, the\n",
      "Time: 0.3639 seconds\n",
      "\n",
      "=== Custom K-Beams Search ===\n",
      "Input: Once upon a time\n",
      "Output: Once upon a time when the world was in a state of flux, the world was in a state of flux, the\n",
      "Time: 1.5691 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class StandardKBeamSearch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"distilgpt2\",\n",
    "        k: int = 5,\n",
    "        max_length: int = 20,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize standard beam search\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the model to use\n",
    "            k: Beam size\n",
    "            max_length: Maximum length of generated sequences\n",
    "            device: Device to run the model on\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.k = k\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "\n",
    "        # Load model and tokenizer\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "        # Add padding token if it doesn't exist\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        num_return_sequences: int = 1,\n",
    "        early_stopping: bool = True,\n",
    "        output_scores: bool = False,\n",
    "        return_dict_in_generate: bool = False,\n",
    "    ) -> Any:\n",
    "        \"\"\"\n",
    "        Generate sequences using standard beam search.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Input token IDs\n",
    "            attention_mask: Attention mask\n",
    "            num_return_sequences: Number of sequences to return (must be <= k)\n",
    "            early_stopping: Whether to stop when all beams are finished\n",
    "            output_scores: Whether to output scores\n",
    "            return_dict_in_generate: Whether to return a dict with additional info\n",
    "\n",
    "        Returns:\n",
    "            Generated token IDs or dict with generated tokens and scores\n",
    "        \"\"\"\n",
    "        # Move inputs to device\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(self.device)\n",
    "\n",
    "        # Number of sequences to return cannot be greater than beam size\n",
    "        num_return_sequences = min(num_return_sequences, self.k)\n",
    "\n",
    "        # Generate using standard beam search\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=input_ids.shape[1] + self.max_length,\n",
    "            num_beams=self.k,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            early_stopping=early_stopping,\n",
    "            output_scores=output_scores,\n",
    "            return_dict_in_generate=return_dict_in_generate,\n",
    "            use_cache=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def decode(self, output_ids: torch.Tensor) -> List[str]:\n",
    "        \"\"\"\n",
    "        Decode output token IDs to strings.\n",
    "\n",
    "        Args:\n",
    "            output_ids: Output token IDs [num_sequences, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            List of decoded strings\n",
    "        \"\"\"\n",
    "        return [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]\n",
    "\n",
    "\n",
    "models = [\n",
    "    \"distilgpt2\",\n",
    "    \"gpt2\",\n",
    "]\n",
    "\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The meaning of life is\",\n",
    "    \"In the future, AI will\",\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    print(f\"\\n=== Testing {model} ===\")\n",
    "    beam_search = StandardKBeamSearch(model_name=model, k=5, max_length=20)\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        input_ids = beam_search.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        # Regular generation\n",
    "        outputs = beam_search.generate(input_ids)\n",
    "        decoded = beam_search.decode(outputs)\n",
    "        print(f\"Output: {decoded[0]}\")\n",
    "\n",
    "        # Generate with scores\n",
    "        outputs_with_scores = beam_search.generate(\n",
    "            input_ids,\n",
    "            num_return_sequences=1,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "        # Extract sequences\n",
    "        sequences = outputs_with_scores.sequences\n",
    "        decoded_sequences = beam_search.decode(sequences)\n",
    "\n",
    "        # Print sequences\n",
    "        print(\"\\nSequences:\")\n",
    "        for i, text in enumerate(decoded_sequences):\n",
    "            print(f\"  {i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "st2CO70JWfoV"
   },
   "source": [
    "## Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539,
     "referenced_widgets": [
      "5675b213a4384943870af14ce20443cd",
      "91202832c5ff4709a981bffb34b61b2d",
      "8d6c0bc1dc38421886a842619ca63866",
      "14ce5b555b2d4f998aec6f5ef24c9620",
      "64ec57a41dbc4a089000036796d0e28e",
      "928ca62128c94eddb27e06b5ad45a735",
      "382083fd97144575a01e28b1fc6aa704",
      "b9aaee97cced4a808f3625f6c6e570ba",
      "3e063b045a2c4371927bfff743c9bbbb",
      "3c9e6c0830e34f0d8cfbf610089bce26",
      "f9a36e818d1a40bf971ae614f1f2fcf5",
      "c2a7b29d0733491582ffe39e22776b22",
      "a48e06978b9940e9965fa689f9e6a597",
      "cc457d49834f473ebc45876d05111327",
      "ac1cd39cc00743239573ff2bf98aa4ee",
      "16a76cb1497149919d2914056e7863c5",
      "d85c41e112ed440f937b349de68a9427",
      "92fb31cb635a4de9824c5817beebf876",
      "e79c7a65641b48aba9c8345ba5186208",
      "e362e76b8c024f689127baf3cabcb392",
      "3496cdab3e174ad5811e8f2d314c310c",
      "a8bbf7215f8c4a4fa5cf45338341dd87",
      "6e834246354e40d1812f84a0435aaa87",
      "94269c7c03ca44dc92676f47e5622556",
      "f71e4564b1ea4db88e478cb9eb23601f",
      "a78e56b513494230b4e4ac8bca39e643",
      "00b378b898f34cfd96cc90e789ea1fbc",
      "42bf52cbe4c14520b14e56b1d0ba33a7",
      "5fd07e30ec52493d9656156a24b9fc6e",
      "a4b5af598f084d868e26f5b5295f4d53",
      "9e9ebfd2731e44d0b2f08e313571d79a",
      "43aae3972a244572aa8b18b003bddedd",
      "3d73293590f9406590dd8f9fed3965fb",
      "b40d2cb057de4b15b83e27b486c5a2f0",
      "3322d2c2a54845cea3346f388637fe66",
      "f1be6a8fd4e44ac8a055bb50daa32a63",
      "92660f8842ad4479b7c0bf78df5de584",
      "f6e5541eeefd43cd9865a8e46ee430da",
      "20d4df3579bc4330a27c4edde68312db",
      "558e64ef8c604e0f8f9aa5f39720b176",
      "f648021ec958444eb28e82178e392348",
      "11bd7ee7a9494c6cb515f837ca18a7de",
      "55115e669a3f47f7837780e65ebf7023",
      "67e3a6589fcf4ae38f6dbe3320c53505",
      "01b998886f7d4600815d1f6b673fae53",
      "89680ae395144d179546e07aa7099605",
      "44f40fd343104bffab449c0fb20b16ba",
      "3db44e1232d64e858d2d56723906e680",
      "31d16f2fa4dd47a58f9e215ba77197ef",
      "b70d652f43344ca5aa7954199f5420b8",
      "4f6551e6420f410b92faa382f48cd732",
      "9338e440236d4bf98ef156a52561534a",
      "42b1a4b59225483086d1e116fc356d63",
      "58b4848ae56848c8aa6bbe34b41da25f",
      "f10d64615a7e46b28a46942f4e286d21",
      "e618a9693c504efda2c5f2a5f6d3c1e7",
      "74159c5d2e2c4fab953fd584f246fbd9",
      "59810f79fa83460c8697eff662bd7c93",
      "dcf70d1548514387a8434d165908ab40",
      "471970d8b4fb4975b7e3ed97bf6c8653",
      "e98bdcddceed47f09256a09f875ccef2",
      "0a94ae5f6a8442e5811545e1b2d7ca02",
      "fe9c0e35db454d6092e06a443c9e6a5c",
      "132ce397daf449ba854e265c8a3ce039",
      "1398c1aafd5740ba9f13347f2aa573d1",
      "5fed7cdc1e434432a91b2cc28d7f4454",
      "99d759391cd94af19f69b71985e78a0d",
      "ee85d24a1bed449eb85ec3515ede811e",
      "c6cf8f15ba824b59845f625a64504960",
      "f4bc20c215f04de78b68956815bc6d5d",
      "b85d0fae7d604f4880e3a677996a5bc1",
      "d9486b81f48545f7ad21d08798f7decc",
      "84a619762aaa40d5a57df8d151ac6270",
      "88a7b457b6e8481e858e6f37e7cac09d",
      "515fdf75309a4160a3042d389cc2cccb",
      "931f2817fb4943ac9d457a78fa08140d",
      "29a163d397aa4f2cb985981c64b95453"
     ]
    },
    "id": "qRnQOr9OWKBc",
    "outputId": "efe794e6-2f0d-4799-d3a2-7a1c673981ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distilgpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilgpt2: 100%|██████████| 2000/2000 [02:57<00:00, 11.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilgpt2 BLEU: 0.24\n",
      "Loading gpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gpt2: 100%|██████████| 2000/2000 [07:25<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 BLEU: 1.03\n",
      "Loading openai-community/gpt2-medium...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5675b213a4384943870af14ce20443cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a7b29d0733491582ffe39e22776b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e834246354e40d1812f84a0435aaa87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40d2cb057de4b15b83e27b486c5a2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b998886f7d4600815d1f6b673fae53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e618a9693c504efda2c5f2a5f6d3c1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d759391cd94af19f69b71985e78a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai-community/gpt2-medium: 100%|██████████| 2000/2000 [13:22<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai-community/gpt2-medium BLEU: 1.29\n",
      "\n",
      "Standard Beam Search Results:\n",
      "distilgpt2: 0.24 BLEU\n",
      "gpt2: 1.03 BLEU\n",
      "openai-community/gpt2-medium: 1.29 BLEU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_standard_beams(models: list, num_examples: int = 100, prompt_length: int = 10, max_gen_length: int = 20):\n",
    "    \"\"\"Evaluate standard beam search on multiple models using WikiText-103.\"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='test')\n",
    "    results = {}\n",
    "\n",
    "    for model_name in models:\n",
    "        # Initialize standard beam search\n",
    "        generator = StandardKBeamSearch(\n",
    "            model_name=model_name,\n",
    "            k=5,\n",
    "            max_length=max_gen_length,\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        # Generate hypotheses and collect references\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "\n",
    "        for example in tqdm(dataset.select(range(num_examples)), desc=f\"Evaluating {model_name}\"):\n",
    "            text = example['text'].strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            # Tokenize and split into prompt/reference\n",
    "            inputs = generator.tokenizer(text, return_tensors='pt')\n",
    "            tokens = inputs.input_ids[0]\n",
    "            attention_mask = inputs.attention_mask[0] if 'attention_mask' in inputs else None\n",
    "\n",
    "            if len(tokens) < prompt_length + max_gen_length:\n",
    "                continue\n",
    "\n",
    "            prompt_tokens = tokens[:prompt_length]\n",
    "            reference_tokens = tokens[prompt_length:prompt_length + max_gen_length]\n",
    "            attention_mask = attention_mask[:prompt_length] if attention_mask is not None else None\n",
    "\n",
    "            # Generate continuation sequence\n",
    "            input_ids = prompt_tokens.unsqueeze(0).to(generator.device)\n",
    "            attention_mask = attention_mask.unsqueeze(0).to(generator.device) if attention_mask is not None else None\n",
    "            generated_output = generator.generate(input_ids, attention_mask, return_dict_in_generate=False)\n",
    "\n",
    "            # Remove prompt tokens from genereated sequence\n",
    "            generated_continuation = generated_output[0][len(prompt_tokens):]\n",
    "\n",
    "            # Decode texts\n",
    "            reference_text = generator.tokenizer.decode(reference_tokens, skip_special_tokens=True)\n",
    "            generated_text = generator.tokenizer.decode(generated_continuation, skip_special_tokens=True)\n",
    "\n",
    "            references.append(reference_text)\n",
    "            hypotheses.append(generated_text)\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        bleu_score = corpus_bleu(hypotheses, [references], force=True)\n",
    "        results[model_name] = bleu_score.score\n",
    "        print(f\"{model_name} BLEU: {bleu_score.score:.2f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "models_to_test = [\n",
    "    'distilgpt2',\n",
    "    'gpt2',\n",
    "    'openai-community/gpt2-medium'\n",
    "]\n",
    "\n",
    "evaluation_results = evaluate_standard_beams(\n",
    "    models=models_to_test,\n",
    "    num_examples=2000,\n",
    "    prompt_length=10,\n",
    "    max_gen_length=50\n",
    ")\n",
    "\n",
    "print(\"\\nStandard Beam Search Results:\")\n",
    "for model, score in evaluation_results.items():\n",
    "    print(f\"{model}: {score:.2f} BLEU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr2Rm5NMoXBa"
   },
   "source": [
    "## Rouge L Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493,
     "referenced_widgets": [
      "a50b17b36ef34b9ba5e2d86d43f58166",
      "3356b08126034191afdc501d0a316f34",
      "7c2ec7e950394c0f9a812ae7f666f1a6",
      "93b3cecaf34d4c45a2f9631a7ff2ee26",
      "18ee7bd7b7f747da88a92437dbb002fe",
      "5c6968bcffa9489fb2bb2e983c100f92",
      "055da82f5c3c42fab611f5aadfa6065b",
      "b1c0bd9aa0aa4d8cb17eadcc2c758f39",
      "c58db225e614400fb392a112e74ae82f",
      "0bd4804850864594bc854e73b263626a",
      "df7d8f5fb897440d95b7da6453824e5d",
      "71ed86bb8aca45559047d099108bcf1e",
      "f27b1b1e638043649cca54798450ef7c",
      "c2e982751f21412d9b47247126d800ea",
      "5207fcdc40ce4ea4a009859ad4996da1",
      "73af1737425d4918bf65ad175eb6f14e",
      "4a9721b51d9346c0ba6cd6176a11498a",
      "8f4c8a8114114d2cafc450e63140e95f",
      "c72eac96d2f24b43b537f6558bef8325",
      "f590b7863abe4b32a3981459883df04f",
      "fe0ffbed9e244a0ba8e55e39f7c80f86",
      "b3610b5bdb5b45698f1326bdd2ce1a9c",
      "71d87706a3af4fa580613a410a4da7fd",
      "cb9f072eee604bb5b8136d118ba52306",
      "0056939677414eceb5fe115aa3b5113e",
      "61b966ad1f9040c994eed26d7f7fb208",
      "aab6536096294bf9b221bd41edd8f5f4",
      "a54c921f453842d2a18c7725f9869f74",
      "bc75949f8e48417891371f2f692fdb4e",
      "cb271ec2f47846169eccd9e00219aaee",
      "8a78153cd5b54486b94a8b69676e3d14",
      "34dd1fd495524a51b5c817461b33691e",
      "da294b8be40d4f8d83777fbc702fd1f4",
      "db130154d0a44170a61c9bff7f4aa977",
      "12d1d0ef99ee4898bc4f461360782b16",
      "d938657949b54dfebca3cdc146618d9a",
      "29e94fafeb2e4e9cb0a87abfaa5aea0b",
      "4b99fd25c09842c1812db97102356e65",
      "2aecfed9a9564a839001f2da371d8fad",
      "656c708dfb414b38ad365947fdf4d0ae",
      "4883c1dbeac6458cb26ee5e1eb055d7f",
      "19594355808349cabf251d198c219601",
      "bfb2cf88134441c8a5871b74eeeda655",
      "c382376bbcd64ae6ae3bd44eb24bbbf9",
      "03ee1814a51b4a19b9251bd5b30c5994",
      "e429329a0fd44c348ed05de4b8f17892",
      "c3e00e105fd347eb9a2724403a1fbaf4",
      "1df281ef161c46488fac78a39ca0b2d7",
      "da01c583844d44ca89a40f45ce85a7a7",
      "9a002a4e8eb34fe6a5869d88b3bed3b5",
      "32dc19936ae641948e89845ff6d95a5a",
      "109b1549914245408b073b927723c6e1",
      "34a586e28a384e868494ccfb89ac5ff5",
      "14a0d75cd4854668b0153a870d1e0b25",
      "4f2883dcd5a24e36b222f65572d38775",
      "d069890842a44b65b2cd680270c20249",
      "78c4af3c95014400a6e63ee39d0f8d68",
      "abefa5e44e0f4d79acbf568ac82588f6",
      "00f3740cf3b141b2a4f3e0e39d571e3f",
      "aa53afdc4df848c6b02852523467edc1",
      "da8d909e8e644c6bbb6fe404ea5d32b2",
      "0cfb1a4b5fac42d99543ed40a273dad7",
      "6766c3ce630f4dcc810d05fb6b12f690",
      "aaef9bd2010c4478aae3b495d04860e2",
      "630159470310424e9a11742aa1c16890",
      "6386a2bb924344be8035f5ea4bd4b00f",
      "b7640dc566404a739f7bd69157ca494e",
      "c356d8705683403a904eecac4439d464",
      "e71b7e29d33c4a8494367b45345ca89e",
      "aa48d25670f24ef6a3ffeab090c03b2b",
      "a579b980e8484da88cb98f915ff91ac2",
      "8dbe6c1e63a147fcb3ed0b8fd862bd9f",
      "8894867c0935457eb5a5bf6feff7bde0",
      "8b19459171fa4d4c95bdb8cd3c30ed79",
      "9a58dde5d0474753953db2ca08247f2b",
      "011c33b8a20a4c5680d8fe21bc06f72a",
      "de8bd65a06234f209ec2c8edf10f57f1"
     ]
    },
    "id": "esSgwXFeoXBa",
    "outputId": "f65d6f1a-9557-4c8a-d09c-69a1c87b60a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distilgpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilgpt2: 100%|██████████| 2000/2000 [02:59<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gpt2: 100%|██████████| 2000/2000 [07:28<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading openai-community/gpt2-medium...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50b17b36ef34b9ba5e2d86d43f58166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ed86bb8aca45559047d099108bcf1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d87706a3af4fa580613a410a4da7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db130154d0a44170a61c9bff7f4aa977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ee1814a51b4a19b9251bd5b30c5994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d069890842a44b65b2cd680270c20249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7640dc566404a739f7bd69157ca494e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai-community/gpt2-medium: 100%|██████████| 2000/2000 [13:30<00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standard Beam Search Results:\n",
      "distilgpt2: 0.08 RougeL Score\n",
      "gpt2: 0.10 RougeL Score\n",
      "openai-community/gpt2-medium: 0.12 RougeL Score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_standard_beams(models: list, num_examples: int = 100, prompt_length: int = 10, max_gen_length: int = 20):\n",
    "    \"\"\"Evaluate standard beam search on multiple models using WikiText-103.\"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='test')\n",
    "    results = {}\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = []\n",
    "\n",
    "    for model_name in models:\n",
    "        # Initialize standard beam search\n",
    "        generator = StandardKBeamSearch(\n",
    "            model_name=model_name,\n",
    "            k=5,\n",
    "            max_length=max_gen_length,\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        # Generate hypotheses and collect references\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "\n",
    "        for example in tqdm(dataset.select(range(num_examples)), desc=f\"Evaluating {model_name}\"):\n",
    "            text = example['text'].strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            # Tokenize and split into prompt/reference\n",
    "            inputs = generator.tokenizer(text, return_tensors='pt')\n",
    "            tokens = inputs.input_ids[0]\n",
    "            attention_mask = inputs.attention_mask[0] if 'attention_mask' in inputs else None\n",
    "\n",
    "            if len(tokens) < prompt_length + max_gen_length:\n",
    "                continue\n",
    "\n",
    "            prompt_tokens = tokens[:prompt_length]\n",
    "            reference_tokens = tokens[prompt_length:prompt_length + max_gen_length]\n",
    "            attention_mask = attention_mask[:prompt_length] if attention_mask is not None else None\n",
    "\n",
    "            # Generate continuation sequence\n",
    "            input_ids = prompt_tokens.unsqueeze(0).to(generator.device)\n",
    "            attention_mask = attention_mask.unsqueeze(0).to(generator.device) if attention_mask is not None else None\n",
    "            generated_output = generator.generate(input_ids, attention_mask, return_dict_in_generate=False)\n",
    "\n",
    "            # Remove prompt tokens from genereated sequence\n",
    "            generated_continuation = generated_output[0][len(prompt_tokens):]\n",
    "\n",
    "            # Decode texts\n",
    "            reference_text = generator.tokenizer.decode(reference_tokens, skip_special_tokens=True)\n",
    "            generated_text = generator.tokenizer.decode(generated_continuation, skip_special_tokens=True)\n",
    "\n",
    "            references.append(reference_text)\n",
    "            hypotheses.append(generated_text)\n",
    "\n",
    "            # Compute rouge score for this seqeunce\n",
    "            scores.append(scorer.score(reference_text, generated_text)['rougeL'])\n",
    "\n",
    "        # Compute mean and set it for this model\n",
    "        results[model_name] = np.mean([s.fmeasure for s in scores])\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "models_to_test = [\n",
    "    'distilgpt2',\n",
    "    'gpt2',\n",
    "    'openai-community/gpt2-medium'\n",
    "]\n",
    "\n",
    "evaluation_results = evaluate_standard_beams(\n",
    "    models=models_to_test,\n",
    "    num_examples=2000,\n",
    "    prompt_length=10,\n",
    "    max_gen_length=50\n",
    ")\n",
    "\n",
    "print(\"\\nStandard Beam Search Results:\")\n",
    "for model, score in evaluation_results.items():\n",
    "    print(f\"{model}: {score:.2f} RougeL Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZu56z_MoXBb"
   },
   "source": [
    "## BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694,
     "referenced_widgets": [
      "5c9a8995bb9349cbaeb27e30b583ee2f",
      "ce8da5c2d86b484ea6c11d414aa21e01",
      "7bee867e2a854406999ee0cba835f83e",
      "98988f7884ba430fad96a21e867b20bf",
      "35b2c7dc14aa436c80d08f2bc4cc7c12",
      "4a68b3e72d014c7aa97b6cf0f2b66374",
      "8a795784a301448eb457a6f3257a9ad8",
      "31f328891d24427e9376230bbc58aeb9",
      "82a2b27576994fddaaa8dabced136e73",
      "da3eff1498a84524bf785d6dc89686eb",
      "93d37a22ebf34ea2a072046bb55ea4d1",
      "264219106bbb41848c720c50cb01dd7b",
      "df7e232ba9e34bc8811a291850525579",
      "c1fa2b71aa6a4c7cb64e9f9a0e10ca08",
      "4d34180eed41469ea2e8da5673034b71",
      "4c2f279eb2884d4ab329e3ffd4e50dae",
      "fe6010eadf434cffa9276e5dbfdd9434",
      "e4840f6941484bd7bfe2b7916ddbcd9d",
      "62b1511c07784aaaaca0d024dafb5066",
      "cf05cf1df13f404e871f8dd16111deaa",
      "3fe69617bcf343299865c6d417aec89f",
      "f3f219ef492a4ca88826d363452215e4",
      "b5a34441aa074786be55968c0a70428b",
      "55c582aaa458485ea5181933ce105fc9",
      "f2c96c98037346f49845f1cf9c2b0e15",
      "fbc40f05885d4cbabca7f7b0cd573e0e",
      "1a2e503fcdf044d3882341ae0c1734c2",
      "16379d9acec1477cb7c26b6186dd433b",
      "193c2ac5e965445bacd6c3218c018311",
      "8afe0d885659489a8c0c8b4ec9647627",
      "aede9e6494e8477bbcf1fb9519b2c0db",
      "01227c8bc5f24181a62b4052d4a923df",
      "0f4af22cb65042c1ad577f8d7759fe66",
      "c4f9078b18e048d4b15c8bf7319e2719",
      "0f1d7e54e1654b5a876d70a3a320cab2",
      "228ed4a444f542c3a1549a4307186543",
      "6f6d4474617a4e4f918e668be52a12b9",
      "d5ed1a36fe2945a5accee9be4289491c",
      "2aa2be236570435bbd0baee5e49f6363",
      "6eab9d9e3ac941a4bf0bdab12cab8d26",
      "b6111a516fd842279bd2cb8aa8ff4f4a",
      "4d1ecb4044c64ab78fa233fc127a4baf",
      "042fa923080c421a87faa240053fc1e8",
      "436c6ee84c274e0dbe6fd4deb1b63e03",
      "0690a96b9509404198ab54a2cac19b5f",
      "64ec1fce573d406c98368a9c58bb070b",
      "c8fe92968f464d7eb20088c91fb35173",
      "47ad629cf70b4793a499f9af23eed7ce",
      "592292998e1a45d6bd0932a25292bc4d",
      "c15ad68e2c874133b2f163d1939b2f47",
      "a78db3a6200c4b4aa74e61e5c704d8f4",
      "e7d7d55adeba47c5afca617045b5bd8d",
      "f251a3c823e84e3b94a5dce45e1f0a20",
      "a55d23551318444e84d8821f8ba91377",
      "6a09bfeba78c49b7ae5492c71bfddf0f",
      "ed2a7864172c46098cc6b7e232ef5672",
      "d0081d767f024f358bdf4a2730ebce22",
      "c10b82b7a6764d6bbccfb11bce895fa0",
      "4fa3198363fc46b8a6f160af2f5ae1eb",
      "7cf76f9494324d3bb96f6eba741b266e",
      "93626887a42444d69e58cef717a733f9",
      "b596c6681c11498299b03f50a4cb83c7",
      "5ab125f2fb144542a72989558897aff1",
      "bdacbaa02da242b79a3b0a1265673d79",
      "44d11b7c565f4a769d3fecc57eee4dff",
      "e0508e63f006426ab20dd02cd4a9e216"
     ]
    },
    "id": "YS5eJ_VQoXBb",
    "outputId": "abe5aa85-8a36-449b-955f-97aab93d8c79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distilgpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilgpt2: 100%|██████████| 2000/2000 [02:57<00:00, 11.28it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9a8995bb9349cbaeb27e30b583ee2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264219106bbb41848c720c50cb01dd7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a34441aa074786be55968c0a70428b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f9078b18e048d4b15c8bf7319e2719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0690a96b9509404198ab54a2cac19b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2a7864172c46098cc6b7e232ef5672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gpt2: 100%|██████████| 2000/2000 [07:41<00:00,  4.33it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading openai-community/gpt2-medium...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai-community/gpt2-medium: 100%|██████████| 2000/2000 [13:52<00:00,  2.40it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standard Beam Search Results:\n",
      "distilgpt2: 0.80 BertScore\n",
      "gpt2: 0.80 BertScore\n",
      "openai-community/gpt2-medium: 0.81 BertScore\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from bert_score import score as bert_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_standard_beams(models: list, num_examples: int = 100, prompt_length: int = 10, max_gen_length: int = 20):\n",
    "    \"\"\"Evaluate standard beam search on multiple models using WikiText-103.\"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='test')\n",
    "    results = {}\n",
    "    scores = []\n",
    "\n",
    "    for model_name in models:\n",
    "        # Initialize standard beam search\n",
    "        generator = StandardKBeamSearch(\n",
    "            model_name=model_name,\n",
    "            k=5,\n",
    "            max_length=max_gen_length,\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        # Generate hypotheses and collect references\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "\n",
    "        for example in tqdm(dataset.select(range(num_examples)), desc=f\"Evaluating {model_name}\"):\n",
    "            text = example['text'].strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            # Tokenize and split into prompt/reference\n",
    "            inputs = generator.tokenizer(text, return_tensors='pt')\n",
    "            tokens = inputs.input_ids[0]\n",
    "            attention_mask = inputs.attention_mask[0] if 'attention_mask' in inputs else None\n",
    "\n",
    "            if len(tokens) < prompt_length + max_gen_length:\n",
    "                continue\n",
    "\n",
    "            prompt_tokens = tokens[:prompt_length]\n",
    "            reference_tokens = tokens[prompt_length:prompt_length + max_gen_length]\n",
    "            attention_mask = attention_mask[:prompt_length] if attention_mask is not None else None\n",
    "\n",
    "            # Generate continuation seqeunce\n",
    "            input_ids = prompt_tokens.unsqueeze(0).to(generator.device)\n",
    "            attention_mask = attention_mask.unsqueeze(0).to(generator.device) if attention_mask is not None else None\n",
    "            generated_output = generator.generate(input_ids, attention_mask, return_dict_in_generate=False)\n",
    "\n",
    "            # Remove prompt tokens from genereated sequence\n",
    "            generated_continuation = generated_output[0][len(prompt_tokens):]\n",
    "\n",
    "            # Decode texts\n",
    "            reference_text = generator.tokenizer.decode(reference_tokens, skip_special_tokens=True)\n",
    "            generated_text = generator.tokenizer.decode(generated_continuation, skip_special_tokens=True)\n",
    "\n",
    "            references.append(reference_text)\n",
    "            hypotheses.append(generated_text)\n",
    "\n",
    "        # Calculate Bert score\n",
    "        _, _, F1 = bert_score(hypotheses, references, lang=\"en\", model_type=\"roberta-large\")\n",
    "        results[model_name] = F1.mean().item()\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "models_to_test = [\n",
    "    'distilgpt2',\n",
    "    'gpt2',\n",
    "    'openai-community/gpt2-medium'\n",
    "]\n",
    "\n",
    "evaluation_results = evaluate_standard_beams(\n",
    "    models=models_to_test,\n",
    "    num_examples=2000,\n",
    "    prompt_length=10,\n",
    "    max_gen_length=50\n",
    ")\n",
    "\n",
    "print(\"\\nStandard Beam Search Results:\")\n",
    "for model, score in evaluation_results.items():\n",
    "    print(f\"{model}: {score:.2f} BertScore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_C4yhxA_oXBb"
   },
   "source": [
    "## Mauve Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 653,
     "referenced_widgets": [
      "5f88d08cf3b1401b9ffd24e5b1e9bf3f",
      "00cc65f7d90e461d802fa0a8a92a7ed5",
      "5dac3ae8305248269b18a3bece4a8d13",
      "72bbab0e130e46c68999ef869150ed72",
      "cad7bc5d322b4dbcba31ea2f1892fcec",
      "4fc5b76a862048c4b9bc073f1d7ce752",
      "e00da0a4669d43bc8447648177b89703",
      "76d3c23b64be490c9fb1d2bfd041000a",
      "30188a46b15841c7aa35c31c0e756b60",
      "649e536c9a3e47f1a5f2b4552d19cc0c",
      "91816b59180d42589003baf263e5afc1",
      "0dcf27df1dda4a01bea737d46b8d2664",
      "2e5b32d8477c424f915962926fd814fb",
      "4c0d38df84a3475f9fca51e2572204fb",
      "4ccd4ef4bf3d48feac89c585aac462fc",
      "a0c57a8a658c4abdb6d2196ad921fdf4",
      "1b0337b2aa574d6b8918cdf7fb1aaebb",
      "57b2d608fded4ceba1617835c18560dd",
      "9111c8dce83d49fea6abdd5bc9f57ca0",
      "f1a79f0c0cf841eaa62c5c82acab432c",
      "5688a4e3294843c4ac2dd0965ac63d1c",
      "7ac626ef23a643e6830ca2f735d97ed4",
      "f8903a5b31ed4645bfd73944658a27ef",
      "7609fd1d678b46c89d9636bfc0c8df1b",
      "0423eb4cc10e4da3849bfa34ccfc5750",
      "383e4975d6794fa0a01b4e18977cb3f0",
      "5617bd9d0e65442b941693827a32329a",
      "c718f700e1574b8f99a7a3942998866e",
      "df9944a91cb544ada77b19e244b79637",
      "6be6386d304b432385f9f16bdb0121ad",
      "4410a64c396d4c97b33d621791ce3d08",
      "6bd9df37bcb846508aa26d568d3b92a4",
      "bd82ec4647f64204af629aacb2af4937",
      "cdc254f3c0c24a0cabb66b5b00592430",
      "43bcd0ff8b8446198a8349c93e0bf967",
      "515907df1f38441c85c5b4959a709107",
      "554cbc6f1ed3406f8bd7a95966e0a995",
      "6b5449a2edc9461981939172da2637a1",
      "ebe613ebc133432a809c5542a1e05968",
      "f0424fa67dee4357ab118146d7b73a90",
      "d64f1d9a39bc43a5bb1041919af37a79",
      "9beee8dda2044886bd24e8cb2ca0ab04",
      "0586ca03dff64b30aad6b48cebf31a4e",
      "9a1865c94a324260a0186e34e3713d46",
      "68ab6c5701a94af884cb04afa670dcd1",
      "95035c96939e457a96f7bd79ad371120",
      "e572132045014ddfac8977100bf29d10",
      "6180c655a1b644b7b636d50b17f44ce6",
      "e7759b868674464889759560875ca81c",
      "ae5e59ca4de549839dcce459bba07080",
      "85b488e4e01847299fa12a62c76d4536",
      "96fd41d55be94b0ca25c1d6340d4a225",
      "0f30293e09b042d7aa7384372b353157",
      "139b7a37553f454995b0f4ae4f0b0eed",
      "2d0d8ab4b5fc4921a40d6d7f95fe33cf",
      "a354216b590c4fdba9532eec1b2c5212",
      "5a11a1953ddf4622a546f8414713b227",
      "e7e4d11a92ee43adb5f8ee806fcefdd2",
      "81bee98461bb4cd482583212456da2c9",
      "bc9343c587a7459787383c6309a1b8ac",
      "8dbb56245da24394ad6a925f0db12ea7",
      "e3be5ab9bc4e462184a919301e4e2fc4",
      "e1aab27873e54a6194081cfc0f886a07",
      "571eb74b36744b08b7c8bbfb62480dcd",
      "b102e7deb03046daa5f26e1c47a81248",
      "aa30df7c57f34bc6adb94aaaf81f3337",
      "a75679a7590247eb9a14a7c042999f27",
      "71d9329618e74b888cb1c66c352503e5",
      "db10b6e7336f41faa5c27383987e305b",
      "544a8475226f4e7bbf7ed02046badde4",
      "43ecbfe1370b498da305761416076dbd",
      "c1bb8d78b26e4986a9ed017ee48cfcc9",
      "92dd4583022946d4b0e1dacd421cf837",
      "725bbc1dccdb414dbba2b2d6ee61bfa3",
      "dd3b718275ba4bafa166388b5c145a6b",
      "0eb6342eaae342a8badc01e0d8091f4b",
      "662fb7fee86b4c2aa2606861484101b6",
      "446dea72d01d4a979a903846e7148f1b",
      "f4db5fd12dae447fa09d5032ab5d08d6",
      "0dd64d6c2fb94775b989038b879ff14e",
      "1ff30ca7ff384add9ca2583b638604f0",
      "c8a4690402c34ea2a55271c5096e80af",
      "b206ec08016d4e37a205ecdbca6d45cf",
      "eb5c25fffebe47299f0481d2e484266e",
      "8ed8a9e9b9ad468ea6168e086c6ecdf2",
      "78f6fcac19f94b2fa127add86ad682f8",
      "f225fa7c53eb48e280e2ce439abac498",
      "74573ab582d846559b6b9508ca2915d2",
      "02463b1cf9e94b9f9b3df49a0fc0f451",
      "8558845e5fef475bb286fd48caef40ab",
      "1c56ae4e20464f9b95966aba3ef6dd2d",
      "3cef75ea43044a6c8724b0473946cabd",
      "ec8363fe725d4cbfbd7bf9a296da504d",
      "3f3a96ee85954a6fadd2fa40a5e90472",
      "4feb33b3851a427fadbf474e4cea85ec",
      "55a8a7f716104d0faa7e9a55cc64012b",
      "4742fe13b3234f85849fef3eeade30d2",
      "6bf7c65fbff04bed90569cb0a31170c2",
      "3f04813419af483d9a09f76d1a7bd289",
      "b4cf3a69aa3e49c289a629dfe242d643",
      "78c5b88292ef4a54a9bbe589234b77fe",
      "586951b2c6234bbea0bcbbffb394b2b9",
      "bc6c2b07b9f74e90a8bd4b8b16d3c81f",
      "2cd58882cafe4e18b8129aa23686eca2",
      "6a7e565c837c4bcf8bd9220a0e588154",
      "ecb12200be9e4039bb89e520a2804db1",
      "2a8f3c75bf914580b280f8d0a655af66",
      "9ef81d7cdc7a4868bee538f5fb191fd8",
      "ffa41b07845b40d9baefc8bf94e457ba",
      "15db4a794ea04f51be7d66096655d62f",
      "d9a74a61b8124581b8250fa06ef2519d",
      "f7abccfdcb4844e096c6f76a5c610963",
      "a5b64b7ac5344fba8da7b024263ebbee",
      "cce5218a46f742bab367d4e51a7f2aa6",
      "72a92ba3106048959419d098ca7e0ee3",
      "39316daba693418390f980e72c90b268",
      "cf15e5da8642473b9896aca8d50969db",
      "3235ddf3ca4b42da8d2e1762a4d26877",
      "db11079c178b43a4a53d2447b609683c",
      "eab20657748048b392aa22398ba9f26d",
      "9648a358b35a42deae87cae341e1aedf",
      "9ec3feefd0094399878c54053a94797c",
      "8227d6709b8d41ada8c7b7ebf27a6688",
      "807c0a96f84245cea06a6d2c56c41011",
      "cf6ab83f1da041c29b67f48bd6b5805d",
      "50127f12808d472bbbb96a3adb71cb5f",
      "4686b9517a214bbeac60cbe6d53169e4",
      "98c5bc9fa5534529a77978707c9c797c",
      "3caec24264e74237ad995d786a4fa92c",
      "3f7e18d855f2421d97712017d9d061fb",
      "60eec6bcf1a148d2bf5ef9b32832db6b",
      "c95bfc6b0160438b9431d07d42e84ace"
     ]
    },
    "id": "96d7SJi7oXBb",
    "outputId": "04512bd1-1a7e-4f6f-ef3b-1d9bc3d6245f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distilgpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilgpt2: 100%|██████████| 2000/2000 [02:56<00:00, 11.31it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f88d08cf3b1401b9ffd24e5b1e9bf3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dcf27df1dda4a01bea737d46b8d2664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8903a5b31ed4645bfd73944658a27ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc254f3c0c24a0cabb66b5b00592430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ab6c5701a94af884cb04afa670dcd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a354216b590c4fdba9532eec1b2c5212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75679a7590247eb9a14a7c042999f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing p:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446dea72d01d4a979a903846e7148f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing q:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gpt2: 100%|██████████| 2000/2000 [07:27<00:00,  4.47it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02463b1cf9e94b9f9b3df49a0fc0f451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing p:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cf3a69aa3e49c289a629dfe242d643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing q:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading openai-community/gpt2-medium...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai-community/gpt2-medium: 100%|██████████| 2000/2000 [13:28<00:00,  2.47it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a74a61b8124581b8250fa06ef2519d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing p:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec3feefd0094399878c54053a94797c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing q:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standard Beam Search Results:\n",
      "distilgpt2: 0.01 Mauve Score\n",
      "gpt2: 0.16 Mauve Score\n",
      "openai-community/gpt2-medium: 0.24 Mauve Score\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import mauve\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_standard_beams(models: list, num_examples: int = 100, prompt_length: int = 10, max_gen_length: int = 20):\n",
    "    \"\"\"Evaluate standard beam search on multiple models using WikiText-103.\"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='test')\n",
    "    results = {}\n",
    "    scores = []\n",
    "\n",
    "    for model_name in models:\n",
    "        # Initialize standard beam search\n",
    "        generator = StandardKBeamSearch(\n",
    "            model_name=model_name,\n",
    "            k=5,\n",
    "            max_length=max_gen_length,\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        # Generate hypotheses and collect references\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "\n",
    "        for example in tqdm(dataset.select(range(num_examples)), desc=f\"Evaluating {model_name}\"):\n",
    "            text = example['text'].strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            # Tokenize and split into prompt/reference\n",
    "            inputs = generator.tokenizer(text, return_tensors='pt')\n",
    "            tokens = inputs.input_ids[0]\n",
    "            attention_mask = inputs.attention_mask[0] if 'attention_mask' in inputs else None\n",
    "\n",
    "            if len(tokens) < prompt_length + max_gen_length:\n",
    "                continue\n",
    "\n",
    "            prompt_tokens = tokens[:prompt_length]\n",
    "            reference_tokens = tokens[prompt_length:prompt_length + max_gen_length]\n",
    "            attention_mask = attention_mask[:prompt_length] if attention_mask is not None else None\n",
    "\n",
    "            # Generate continuation sequence\n",
    "            input_ids = prompt_tokens.unsqueeze(0).to(generator.device)\n",
    "            attention_mask = attention_mask.unsqueeze(0).to(generator.device) if attention_mask is not None else None\n",
    "            generated_output = generator.generate(input_ids, attention_mask, return_dict_in_generate=False)\n",
    "\n",
    "            # Remove prompt tokens from genereated sequence\n",
    "            generated_continuation = generated_output[0][len(prompt_tokens):]\n",
    "\n",
    "            # Decode texts\n",
    "            reference_text = generator.tokenizer.decode(reference_tokens, skip_special_tokens=True)\n",
    "            generated_text = generator.tokenizer.decode(generated_continuation, skip_special_tokens=True)\n",
    "\n",
    "            references.append(reference_text)\n",
    "            hypotheses.append(generated_text)\n",
    "\n",
    "        # Calculate Mauve score\n",
    "        score = mauve.compute_mauve(\n",
    "            p_text=hypotheses,\n",
    "            q_text=references,\n",
    "            device_id=0,\n",
    "            max_text_length=256,\n",
    "            verbose=False,\n",
    "            batch_size=16\n",
    "        ).mauve\n",
    "        results[model_name] = score\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "models_to_test = [\n",
    "    'distilgpt2',\n",
    "    'gpt2',\n",
    "    'openai-community/gpt2-medium'\n",
    "]\n",
    "\n",
    "evaluation_results = evaluate_standard_beams(\n",
    "    models=models_to_test,\n",
    "    num_examples=2000,\n",
    "    prompt_length=10,\n",
    "    max_gen_length=50\n",
    ")\n",
    "\n",
    "print(\"\\nStandard Beam Search Results:\")\n",
    "for model, score in evaluation_results.items():\n",
    "    print(f\"{model}: {score:.2f} Mauve Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpMVUNn0qMv5"
   },
   "source": [
    "# Testing different Window Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaDuecp_qnjs"
   },
   "source": [
    "## Pre-trained GPT-Medium model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "eb64c432409749e6aaca13320b2f2155",
      "8f8d98cf94c4451694e5c0c673c8ed76",
      "564037bdf7c84b5897504617dff43fec",
      "5172d7ff9970445db3dc6d9073a421e0",
      "cb2791de93a245b0a46485d6172a9c3f",
      "2d7b9e6b9a87459d8828dc16a637f6de",
      "6eb13739f0e6469e9fcd45e1907eb4ef",
      "5c6a545d54fa4bfa8e5533ffa87cdee2",
      "1f2eeb88d3664b0faa36a9df22689a14",
      "054c994b12114bf8875938d3fbdac315",
      "f13f2ea76bbb4e65927c6331cf6ae96b",
      "4329bbc83f614c30a5b5335ca6800b20",
      "c962ba8b93df49c58b2e21c2d7ad4b0c",
      "f90899b807454cee8ade82bd9f6802e8",
      "dec3d2739f4e4b7f863b693e1ebdbfa9",
      "730783b44fdd4a3c9c33ab08af8617a4",
      "e53085bc09294f81928e3c09c8217424",
      "5f964f21ead84cf491a5fbff70e53661",
      "cc32f640af894fcc9e2dd128fe1eb3b6",
      "6ca438388bb74e94867b1bfb61044670",
      "eb77ccbb3b9d4b879ec13741b770cc6b",
      "6cf54c20c01f4b8eb7db12ed82c1752c",
      "42b4ca65152846678b86f7934b6c6960",
      "529252d4893743e992192463782f3b39",
      "59600c087c9c4c5e94a5a3bde39f273e",
      "23450dcaa9184defa614562045d9704a",
      "652a615f19b34260a132c2e3a553dba9",
      "67a41fb8d012405da55197887f2dc977",
      "97a372b488a84560bb293f5642d186cb",
      "2b2a298adebe464a8f351cc77cd2ea74",
      "db019b98cc3e4f38a4acea4e5cd20d98",
      "73da5b6c3b0743fa9c72584e1f1098ff",
      "ab4358ce40fa474aaa406baead9db5f7",
      "337cdabd91164f0994c2cd9fd7a97dd4",
      "902225d9463042388ed7fa0e5c0fcadb",
      "b6b761cff9fb4921be7a5604032905bd",
      "e29076238e0b458aaaefb30756e92372",
      "1d4a993e5f26409fa64e8c63a5d32c4c",
      "6ae791c0b99d40b29faca5557c1ded04",
      "67d29b1e53d74253b3ba81ad63b8eb51",
      "3028e26918884c7396c858a24cef7177",
      "7cd23c08ba8c498f9f26f62cfdd74541",
      "bbd5c964e7b14d76891eee209407cfca",
      "4749cce2242b4a57ac6970d22c31a4b2",
      "9f2eda0a8eeb492a8b286d291441fa28",
      "d18fc7c6bf564ecebba4b8193a203704",
      "dc24488bb1b84bc7a6e3e2e80b3d4f07",
      "8c7d13a0e9984b6a86398c6ea1c3b0c3",
      "a23770afe28a48969959a88472b98745",
      "b439ee53a0b340ab89b2b34e6f7ed19a",
      "ff28148885bb4d9db48cdf2d1cf9d667",
      "15e7c7f169494e96b3d7ce3502afb16b",
      "aa7045f299e7495d96e7d5d7048ce2c7",
      "84c1997c7baf4d30a5e0ccdcb5a2456d",
      "039a8375c5e647798759b304239c4005",
      "02f6120873c24c3888f32bd7a8874207",
      "02b0c35b870a40ffaa5f34ce63091324",
      "2cd3f606dfd146a6b0afb0d6169f9671",
      "6441283074f9429caf8512812e4e0026",
      "b1c3c1c408574b4eb9789d5498181d80",
      "2517414714bf45dc94eefb86633e3864",
      "95f22299c10d40f1868ac1cb2653b468",
      "9662d5612280423591a3bf7981943360",
      "4f4d65b191354f22bce66b8c4335f750",
      "d9846d755d1a4bc28a7438b93a26ace7",
      "3a2e50d637f3491fb680588a47ff4c27",
      "31bb63d3f0f749ee94138f853cae3b39",
      "d6d07d6d4bbc4c21a7b22279a0379c7d",
      "adbab817ba3049c69843016117ddf0d7",
      "baef69dce40445a682e07722ff2c2c32",
      "d94571f2b31141148d78bcdfd31e331a",
      "000c6dc493ae47e69e0fe2fb3515184d",
      "bfac4a1482f94607aef589ffec8aec44",
      "30ca2705cfe84461b793ed28df26c779",
      "bbffe76d76174d1b9bcf06938bb92878",
      "5fb1aaeaeaed4440aa8e8bc7664a1902",
      "0374b2270317431e84ccd3da194c2a62",
      "fe70ddbff0a048acb9a268818c2ff9f3",
      "14ce58a021b14c13ad36e0fcdbd59838",
      "80895baff1084bf5bae7cc93a7528dfd",
      "607c84dfcf36451e93a16e2af2829fc2",
      "e98225edda6f4696bc1f4f01883a3778",
      "c41a7502899e498f9f1b2f80c2cd8712",
      "876e33b498094edbb4a8418b7eea8e07",
      "fb0f4a28332d4b09a029cce8d3ead829",
      "f62393d3d5ce420499ffa8fe3009d71c",
      "a0deac0cb359421ebd9613a884c815c5",
      "27dee2f5a7ac4a6990be54a0ac71e5fc",
      "30837ea7e85842898af565a0c51feac4",
      "335bc4be3dbc494ba86080599ad87079",
      "ce4c3fab34ca41a4a9fc8440dea9549b",
      "eddc9d7a9cee49a5ab42a5d6b315a055",
      "257331709a67482a9e87ebe03730b9da",
      "82fbbed6049e44a4bc92b6541ab9f4db",
      "0ce3a74fcbde4533ac8a939d4494a899",
      "edb2d892c5584e52acb0778e6a0ae32f",
      "e9b66cd368824f2a8e12b212ea3f097c",
      "d1cf72951a0749778833dd7e369b51ad",
      "316e601e74f9422b9d51e117aefbfd81",
      "f6590ce55e0549c0a30646f6349e9aac",
      "2e30afaa46e743d7adc684a9c181c564",
      "143841c76dc84ab78abd2dd919005309",
      "057f43fe8d62415eab28b86d536dd07b",
      "d5dfcf21b857436085148ddb4c9679be",
      "1a164fac626a4841a7ea9a8acf11d651",
      "d7c6ea53dccb404c9c074dfe7f97bc68",
      "4b0d5d4b5a66417d924843168425a39a",
      "4b085076d1ad4d2db38b7409179fd81e",
      "f20967a5621044da9fa1f0c4b53388a0",
      "7c29fb68ef1549eebb9912f49044ea5d",
      "2e110d22abee44faa5c37692512b151e",
      "d68b9803f74944dc92b35b546435ff69",
      "24f51d7b4d974a0e88a1cc14543fe567",
      "971a4faa6a2944af94ef66b071e46521",
      "1a349c59918d4c66a3416c381716c6ef",
      "ccf392307aef458db6e58474b6b8d4fb",
      "a90c4dcdaa1e4a4eba8528bc9b6b867e",
      "54327405b1ce4fefbd017f92d0fd0405",
      "f6ebe81de81f4b52b74bbd7e5130f675",
      "bb323d51076246ada7f1c271be8c0a40",
      "6d9c2203667d48709acd44a8739aa619",
      "b0fd6c1a32914c8f985c3ed3d3b4a1b1",
      "6bf4b5af23ae438db167a2c0ea53562e",
      "14bac6f84c88441a8a4af50498c69b1a",
      "72cef0dcfcaf411c97817e4911b6badb",
      "bbe4f05372a94a879d2969c4badeea2c",
      "f02ca1b5d72b42e099f23ae9fe5b7e5d",
      "454b0f940f1f475f9e0808fcf3832f6c",
      "0853f2be7abe48229bf6eb2da701d32b",
      "41c2f6e15257493aa0913efb00ce2f6f",
      "6cf65d07e05a4f048db85f57a90f1594",
      "935b334b2f624b11b930f037a54dbf53",
      "267c895e94374a9bb140f406398e1b18",
      "3ca47c1ec18c4997a1f762c32f0a1ca2",
      "f008a6d73ca8434d984b1724fc04927a",
      "a71b09e01014414c80507efd8728bf84",
      "ef022f3c76624a40bc61388805aa7c1f",
      "f6e664f5663446bbbc74bba04e518a91",
      "80c3bf1067ae459ab93fee07ab755125",
      "2bfface2090049d48d0efc91283b427f",
      "f708e3b76149423fb49dcde9333a584a",
      "ad6eb84380064823a124417cc08dbc0c",
      "3f4315eb957a4e44984694a9eb601fe5",
      "c5e823d6aeb0404982a3332ce3e47cba",
      "631c0611540e4183a913d1f7b7b859af",
      "ccbdaf3756084c5d90d220e66e93cc9c",
      "13fdb28948cc43368a86caf84719066f",
      "b008c11bd44241998d980b842de21c72",
      "8b78067d11734c2eb1990d5db1e5c522",
      "0f1a06f57c30406e8f74f1e692219cee",
      "e7fcb5de84e545f6a7af40441f9a221c",
      "713d5f8f03414b06b8aeb4a46f87799e",
      "6f4d088750274b8497a6f6ae442ab710",
      "f72a9c4fa1d14ad79fe82e2d1edfe3ca",
      "f723e33e588645928b4a99159f874a2f",
      "e081cb529a654859bfdc656498ca10ec",
      "ed4973e2985a45429a1d936e8f73870b",
      "c171fe3a93164ae8a8033238f50b3986",
      "9c12e922df5e4b11a8f8922a8d9f425b",
      "9957806ffac4476f9635f8eff432528f",
      "35d114f0cb96487b86524f4ddd5df848",
      "9f6913a7156b484c9e37206179fadf5c",
      "e7a9dca79e964d99ab3eb9226729bcc3",
      "dcab05d0d3fa43929856025f93b465c0",
      "58ca0dbdc0ea408f8daaef402bb990d3",
      "d0e536aec95b4843bba3d4a94015612c",
      "f3b62d83888d4812802c66412985efb4",
      "86d4a43bf4f5456185a321c2b2933089",
      "11e3664a75284a67b962679a967db343",
      "02e05f428ca544df8ae50781880771ad",
      "54a2c4b86d1e44289c6ae1d481079daa",
      "3c590376dbb64187bb7d9db1038e6120",
      "6cc531115d5b456a8868dfdc504dda65",
      "e49b713aa0a041cea3e99ce0c3e0e7d4",
      "a1c318996998432787d4db7b562bd656",
      "0f6d72a53ff84b5882ec4d5b664d6b98",
      "5b454055251b43fe9ef982cbb80d2d4e",
      "86c10ac8507b4a1282c4abf7c3eb3afa",
      "be9a47df5294469c97e056e49f5d51db",
      "f96ec0135e2f439cbbfdf516fa913c49",
      "d214a497b1184d44ac5aa49c88455f18",
      "e305299f003c46949d6275853ac508e7",
      "db720a0d350b41248f4838c6f6036287",
      "558b3f6cb0b2473281f4b1e3c3005fea",
      "2a9bdc683eac449b9691f8fdeb46441b",
      "7019ab2dd01148ac9c85a42bbde0111d",
      "fc3cf21bfc1749948e550651da1e4cd2",
      "de5f7c9ff3664477b90bd3770cff35c5",
      "9f722563544b4f7d95b2f970b80655e1",
      "ff7bdcd0da3e44c9825a1eb97bf7ddba",
      "109621e20a384c2aa760094f0fa5beb0",
      "49f50d459179424ab16bcc4cd8477cf1",
      "92e9baac18af42ce9939fd1e00533a61",
      "acef52ed39e145b29c81956b56c10154",
      "76b1e30d60144110bf5f6ad5483bdeef",
      "bb1d1d594fe84481a1d662b8221b264a",
      "98472ebc39d24caf96fb4a829881255b",
      "f9260f51c63641a59e95100bc6774371",
      "a28eab02a91c40faa3a93f89f9ac9158",
      "8e5f5930a44d4ce1a4781be862396f23",
      "8f66d0f525e64f879886e8efed804128",
      "bbde87d39e6e4b3580c6d58448db508b",
      "68dd8821e0a546fdb55adaf3caf22a8f",
      "f6a5004587924ca284a33a13f0606972",
      "cda7d57174704d74ada649cfc5b39a51",
      "9ba5794197bf497d90804c9fd1746cfd",
      "12dbaca019d34a6388dab159d736bb4c",
      "9e2305c4a61346fabcbae7b921b2ac52",
      "6ff8bbd92a454ff9bd2a3a6b6ce26fba",
      "1ae1464c992f4e619bcc6f1388a688ae",
      "16cb9ded6e51447889daefdd854b62cc",
      "d776919964524863add4f01c9957712c",
      "5a4a7b77bddb4bf783ac79ea2ee4753b",
      "07e059d5cef8422fabec0376a952c587",
      "8496d5aa8e74422bb5fd8f0d2a432580",
      "0c3358a7dbe74446ba35203f187ed46c",
      "d28c24ec829f472f9866d1ddefd2a5ea",
      "bee01b3d43624b5c85460c8f47518dd4",
      "c1dc8ef6abf74fcd845e00b3e06f8320",
      "e45c2af5cbf64b5ca3a709f15bc3b5a6",
      "00a215a4e7624bdab9003d626ec4cc4f",
      "da7b508501c147b1b5af5c3027dc22bf",
      "924ae6eaf247423382ca27ae95181197",
      "173345f836304858bc4111e0a17f9755",
      "b6642e15e86a4aee8ebf3b131df39e17",
      "b1ff351769b543ffa4bad53e6e8c99a9",
      "e95efaf8bc6d44a9ae4c2126b2908731",
      "901c1cf32f8047c89365318a499cb199",
      "897b4b3acd994fcb9bcc4c36a5f49b4e",
      "899cc3436dae4dd0a53e319ea9c34bd0",
      "57864fe534454250919cb9755c84d6ed",
      "6727fe56eacd45c8964dc349deb37b62",
      "05f7a074975c44dcbd98e9cd6a97b459",
      "3da72caf72654f8d849e7091180df5d0",
      "6c3c15d9b0cd40658532940230a4cd3a",
      "7c3c8f32a99e476cad17ff61366600bc",
      "c85e366ae40c49728e73d47e1985476f",
      "15236da74c5140809c95c0d13e3339ad",
      "865830a5f6824c6f9b7549977e840609",
      "57e1d419fc9a45928749a2c912ab8a7b",
      "c896c6aeb59e4ceeba46e80a412ebddb",
      "b5d3ae53edf54965ac2fa471a8c6f5a9",
      "c8bb9f419ebc4e179d56af2eb06c004b",
      "48e8a1496a4740fc8b027a97249c49bb",
      "36a3a7440c814e11a4484ff0d2d26aed",
      "babfa08e35c34d059a3bc31706db75e5",
      "052bfc228b3449679952906fbf573525",
      "40749e04478940c8944f8c1e61f8787e",
      "eed1370c487e4004a7c259793b779297",
      "e5e17f6d8d1b4bfe96e7bfc5db05b167",
      "40bf884271a94a39af8df11c9157d2ca",
      "8d83985a113b44da86fb38c8da8e6f11",
      "57d29ebebbc74cc9b9b0106812d60c21",
      "1c4807df6ba44016826572ed8275c66d",
      "1b2bb05b922d47e3b50ce8ef673ecbb4",
      "039e776ea9ce438daa26b60b9b28b74f",
      "9ba326cac391429486308c03cbaef5bb",
      "0bae970a692f41c4adac6f85985d4dc8",
      "b2f6b413418644df9d327b7a1984124f",
      "3eb0b4e73cab4377a46191b4c2b77820",
      "a967ffbed4384585abb954265585dba5",
      "075d84761e79468ca272dbad82a754f4",
      "7f8577a68020442bb53181b36267489b",
      "c9fc1474e39441b5b41ff1ebc4464192",
      "a1686618030f4dd1a945de42f23a992b",
      "304f216439de4b4ab751818a63ec56d0",
      "4501240a101346e486983efa4e2a14e0",
      "8a70436edd1644e58ce5502ca37c2df0",
      "3fdf14e6f18b45b69b13824ac3139673",
      "78e2b83f0a73409288317be7524286b7",
      "dae0d83e4da14380aa1e1421d5c7e86d",
      "9bf8219a934e42bb95dde1bcfb53c197",
      "2a65f471616541db82fe485866e4e8bd",
      "c92a6188438140f196512cf243f007f2",
      "3eb7011e31274176bfa69206ac44c072"
     ]
    },
    "id": "IT6Atf_qsJtC",
    "outputId": "ae1a9150-a37a-4e63-8946-fd987322f043"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb64c432409749e6aaca13320b2f2155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4329bbc83f614c30a5b5335ca6800b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b4ca65152846678b86f7934b6c6960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00002.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337cdabd91164f0994c2cd9fd7a97dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00002.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2eda0a8eeb492a8b286d291441fa28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f6120873c24c3888f32bd7a8874207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bb63d3f0f749ee94138f853cae3b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe70ddbff0a048acb9a268818c2ff9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model openai-community/gpt2-medium...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30837ea7e85842898af565a0c51feac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6590ce55e0549c0a30646f6349e9aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e110d22abee44faa5c37692512b151e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0fd6c1a32914c8f985c3ed3d3b4a1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267c895e94374a9bb140f406398e1b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e823d6aeb0404982a3332ce3e47cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f723e33e588645928b4a99159f874a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded. Starting Computations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Window Size: 2: 100%|██████████| 2000/2000 [37:15<00:00,  1.12s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e536aec95b4843bba3d4a94015612c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b454055251b43fe9ef982cbb80d2d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5f7c9ff3664477b90bd3770cff35c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28eab02a91c40faa3a93f89f9ac9158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae1464c992f4e619bcc6f1388a688ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a215a4e7624bdab9003d626ec4cc4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6727fe56eacd45c8964dc349deb37b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing p:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8bb9f419ebc4e179d56af2eb06c004b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing q:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for Window size 2 is 0.21936078374996132.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Window Size: 4: 100%|██████████| 2000/2000 [49:29<00:00,  1.48s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4807df6ba44016826572ed8275c66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing p:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1686618030f4dd1a945de42f23a992b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing q:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for Window size 4 is 0.2603922832563385.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Window Size: 5:  78%|███████▊  | 1557/2000 [43:42<12:17,  1.66s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import mauve\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_kbeams(model_name, window_sizes: list, num_examples: int = 4000, prompt_length: int = 10, max_gen_length: int = 50):\n",
    "    \"\"\"Evaluate custom k-beams search on multiple models using WikiText-103.\"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='test')\n",
    "    results = {}\n",
    "\n",
    "    # Load model and tokenizer outside the loop as we are not training\n",
    "    print(f\"Loading Model {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    print(f\"Model Loaded. Starting Computations...\")\n",
    "    for window_size in window_sizes:\n",
    "\n",
    "        # Initialize custom beam search\n",
    "        generator = CustomKBeamSearch(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            k=5,\n",
    "            initial_window_size=window_size,\n",
    "            max_length=max_gen_length\n",
    "        )\n",
    "\n",
    "        # Generate hypotheses and collect references\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "\n",
    "        for example in tqdm(dataset.select(range(num_examples)), desc=f\"Evaluating Window Size: {window_size}\"):\n",
    "            text = example['text'].strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            # Tokenize and split into prompt/reference\n",
    "            tokens = tokenizer.encode(text, return_tensors='pt')[0]\n",
    "            if len(tokens) < prompt_length + max_gen_length:\n",
    "                continue\n",
    "\n",
    "            prompt_tokens = tokens[:prompt_length]\n",
    "            reference_tokens = tokens[prompt_length:prompt_length + max_gen_length]\n",
    "\n",
    "            # Generate continuation seqeuence\n",
    "            generated = generator.generate(prompt_tokens.unsqueeze(0).to(model.device))\n",
    "            # Remove prompt tokens from generated text\n",
    "            generated_continuation = generated[len(prompt_tokens):]\n",
    "\n",
    "            # Decode texts\n",
    "            reference_text = tokenizer.decode(reference_tokens, skip_special_tokens=True)\n",
    "            generated_text = tokenizer.decode(generated_continuation, skip_special_tokens=True)\n",
    "\n",
    "            references.append(reference_text)\n",
    "            hypotheses.append(generated_text)\n",
    "\n",
    "        # Calculate Mauve score\n",
    "        score = mauve.compute_mauve(\n",
    "            p_text=hypotheses,\n",
    "            q_text=references,\n",
    "            device_id=0,\n",
    "            max_text_length=256,\n",
    "            verbose=False,\n",
    "            batch_size=16\n",
    "        ).mauve\n",
    "        print(f\"Score for Window size {window_size} is {score}.\")\n",
    "        results[window_size] = score\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "window_sizes = [\n",
    "    2,\n",
    "    4,\n",
    "    5,\n",
    "    6\n",
    "]\n",
    "\n",
    "evaluation_results = evaluate_kbeams(\n",
    "    model_name='openai-community/gpt2-medium',\n",
    "    num_examples=2000,\n",
    "    prompt_length=10,\n",
    "    max_gen_length=50,\n",
    "    window_sizes=window_sizes\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "for model, score in evaluation_results.items():\n",
    "    print(f\"{model}: {score:.2f} Mauve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfaDr4n3qW7l"
   },
   "source": [
    "We stopped testing over with window size of 4 as the results were sufficient for making a conclusion in our opinion."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "I_j8McAWlyah",
    "2yBdMpG4WTw4",
    "v2e8QNiOV9s3",
    "Z7T58QiCV6Sz",
    "v-4q2YZ4V3qb",
    "A3O3d7kKoAku",
    "trSgN-Z7WaIT",
    "st2CO70JWfoV",
    "lr2Rm5NMoXBa",
    "VZu56z_MoXBb",
    "_C4yhxA_oXBb",
    "IpMVUNn0qMv5"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
